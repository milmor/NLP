{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The MIT License (MIT) Copyright (c) 2023 Emilio Morales\n",
    "#\n",
    "# Permission is hereby granted, free of charge, to any person obtaining a copy of \n",
    "# this software and associated documentation files (the \"Software\"), to deal in the Software without \n",
    "# restriction, including without limitation the rights to use, copy, modify, merge, publish, \n",
    "# distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the \n",
    "# Software is furnished to do so, subject to the following conditions:\n",
    "#\n",
    "# The above copyright notice and this permission notice shall be included in all copies or \n",
    "# substantial portions of the Software.\n",
    "#\n",
    "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, \n",
    "# INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND \n",
    "# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES \n",
    "# OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN \n",
    "# CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/milmor/NLP/blob/main/Notebooks/16_GPT.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />\n",
    "    Run in Google Colab</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ty7Z3RRm2v0P"
   },
   "source": [
    "# GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ryOL1_XG4ABQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.0.1+cu117'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' # Disable tensorflow debugging logs\n",
    "os.environ[\"KERAS_BACKEND\"] = \"torch\"\n",
    "import keras_core as keras\n",
    "import torch\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import random\n",
    "\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ryOL1_XG4ABQ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f16f91f0190>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(77)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.- Conjunto de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text_file = keras.utils.get_file(\n",
    "    fname=\"spa-eng.zip\",\n",
    "    origin=\"http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\",\n",
    "    extract=True,\n",
    ")\n",
    "text_file = pathlib.Path(text_file).parent / \"spa-eng\" / \"spa.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "translate English to Spanish: I won't take those pills. No tomaré esas pastillas. <eos>\n",
      "translate Spanish to English: Esta cuerda no es lo bastante fuerte. This rope isn't strong enough. <eos>\n",
      "translate English to Spanish: She played the piano with enthusiasm. Ella tocaba el piano con entusiasmo. <eos>\n",
      "translate Spanish to English: Creí que Tom estaba muerto. I thought that Tom was dead. <eos>\n",
      "translate Spanish to English: Habrá una recompensa para quien encuentre a mi perro. There will be a reward for the person who finds my dog. <eos>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "237928"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(text_file) as f:\n",
    "    lines = f.read().split(\"\\n\")[:-1]\n",
    "    \n",
    "text_pairs = []\n",
    "\n",
    "for line in lines:\n",
    "    eng, spa = line.split(\"\\t\")\n",
    "    text_pairs.append('translate English to Spanish: ' + eng + ' ' + spa + ' <eos>')\n",
    "    text_pairs.append('translate Spanish to English: ' + spa + ' ' + eng + ' <eos>')\n",
    "\n",
    "for _ in range(5):\n",
    "    print(random.choice(text_pairs))\n",
    "\n",
    "len(text_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "translate English to Spanish: Nobody knows why he turns down my help. Nadie sabe por qué rechaza mi ayuda. <eos>\n",
      "translate Spanish to English: Tom casi nunca desayuna. Tom almost never eats breakfast. <eos>\n",
      "translate Spanish to English: Tal vez sea demasiado tarde. Perhaps it's too late. <eos>\n",
      "translate Spanish to English: Nosotros estamos viviendo en la era de la energía nuclear. We are living in the age of nuclear power. <eos>\n",
      "translate Spanish to English: Si nos apuramos, creo que la hacemos. I think we'll make it if we hurry. <eos>\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    print(random.choice(text_pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Conjuntos de entrenamiento, prueba y validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "_LIiVqBco41j"
   },
   "outputs": [],
   "source": [
    "random.Random(434).shuffle(text_pairs)\n",
    "num_val_samples = int(0.15 * len(text_pairs))\n",
    "num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
    "train_pairs = text_pairs[:num_train_samples]\n",
    "val_pairs = text_pairs[num_train_samples : num_train_samples + num_val_samples]\n",
    "test_pairs = text_pairs[num_train_samples + num_val_samples :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RIz-c8Rao1fV",
    "outputId": "d05acafd-41e4-4d59-df53-e2df9b96bbfc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "237928 total pairs\n",
      "166550 training pairs\n",
      "35689 validation pairs\n",
      "35689 test pairs\n"
     ]
    }
   ],
   "source": [
    "print(f\"{len(text_pairs)} total pairs\")\n",
    "print(f\"{len(train_pairs)} training pairs\")\n",
    "print(f\"{len(val_pairs)} validation pairs\")\n",
    "print(f\"{len(test_pairs)} test pairs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "iB8kP0jRSiG4",
    "outputId": "f1ebfa12-fccf-4a18-81f1-5bd4326c9fdc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'translate English to Spanish: The pay is terrible. El pago es terrible. <eos>'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pairs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NWW4RxWhpAZf"
   },
   "source": [
    "- Crea vocabulario y define tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "7-V2bNQqpLTE"
   },
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import vocab\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "0viwLWSUpGrk"
   },
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer('basic_english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "qpTS_ktRo_xd"
   },
   "outputs": [],
   "source": [
    "def build_vocab(text, tokenizer):\n",
    "    counter = Counter()\n",
    "    for string_ in text:\n",
    "        counter.update(tokenizer(string_))\n",
    "    return vocab(counter, specials=['<unk>', '<pad>', '<eos>'])\n",
    "\n",
    "\n",
    "vocab = build_vocab(train_pairs, tokenizer)\n",
    "vocab.set_default_index(37546) # evita error <ukn>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RlV0maHzt88t",
    "outputId": "fd691500-97f3-483c-ee8b-f3a9dd97fe61"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37535"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(vocab)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "WX9CyCiZq8gT"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "166512"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxlen = 64\n",
    "\n",
    "def data_process(text):\n",
    "    data = []\n",
    "    for raw_txt in text:\n",
    "        tensor_ = torch.tensor([vocab[token] for token in tokenizer(raw_txt)],\n",
    "                                dtype=torch.long)\n",
    "        if tensor_.shape[0] < maxlen:\n",
    "            x = tensor_[:-1]\n",
    "            y = tensor_[1:]\n",
    "            data.append((x, y))\n",
    "    return data\n",
    "\n",
    "\n",
    "train_data = data_process(train_pairs)\n",
    "val_data = data_process(val_pairs)\n",
    "test_data = data_process(test_pairs)\n",
    "len(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mN_d_pESTAkZ"
   },
   "source": [
    "## 2.- Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "cW6E9jkJTe1o"
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "PAD_IDX = vocab['<pad>']\n",
    "EOS_IDX = vocab['<eos>']\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def generate_batch(data_batch):\n",
    "    x, y = [], []\n",
    "    for (x_item, y_item) in data_batch:\n",
    "        x.append(torch.cat([x_item, torch.tensor([EOS_IDX])], dim=0))\n",
    "        y.append(torch.cat([y_item, torch.tensor([EOS_IDX])], dim=0))\n",
    "\n",
    "    x = pad_sequence(x, batch_first=True, padding_value=PAD_IDX)\n",
    "    y = pad_sequence(y, batch_first=True, padding_value=PAD_IDX)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size,\n",
    "                          shuffle=True, collate_fn=generate_batch, \n",
    "                          num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size,\n",
    "                        shuffle=True, collate_fn=generate_batch,\n",
    "                        num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size,\n",
    "                         shuffle=True, collate_fn=generate_batch,\n",
    "                         num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u_N6IRYsT4ry",
    "outputId": "a1d1522d-c8db-493b-fc77-e3549aeeb40a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151 ms ± 5.76 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "train_batch, target_batch = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "aauQX2tFXD8K"
   },
   "outputs": [],
   "source": [
    "train_batch, target_batch = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ROw_9EjsT2Et",
    "outputId": "37555466-8faa-49bf-898e-e7374d2c66db"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([128, 37]), torch.Size([128, 37]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_batch.shape, target_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   3,    6,    5,    4, 5207,  384, 5373,  110,  234,  157, 2234,   18,\n",
       "         454,  110,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_batch[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PZ7f4DHJreIj"
   },
   "source": [
    "## 3.- Modelo\n",
    "- Definir auto atención producto punto con máscara:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mbox{MultiHead}(Q, K, V) = \\text{Concat}(\\mbox{head}_1,\\mbox{head}_2,\\ldots,\\mbox{head}_h)W^O,\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\mbox{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V) = \\text{softmax}\\left[\\frac{QW_i^Q(KW_i^K)^T}{\\sqrt{d_k}}\\right]VW_i^V,\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dT6uffV0rdh_",
    "outputId": "1f783aa2-47b4-4c1f-b243-e512ee31a3ed",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0978, -0.7630,  0.1265,  ..., -0.1886, -0.6295, -0.4667],\n",
       "         [ 0.0978, -0.7630,  0.1265,  ..., -0.1886, -0.6295, -0.4667],\n",
       "         [ 0.0978, -0.7630,  0.1265,  ..., -0.1886, -0.6295, -0.4667],\n",
       "         ...,\n",
       "         [ 0.0978, -0.7630,  0.1265,  ..., -0.1886, -0.6295, -0.4667],\n",
       "         [ 0.0978, -0.7630,  0.1265,  ..., -0.1886, -0.6295, -0.4667],\n",
       "         [ 0.0978, -0.7630,  0.1265,  ..., -0.1886, -0.6295, -0.4667]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, maxlen, n_heads=4, bias=True):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.scale = (dim // n_heads) ** -0.5\n",
    "        self.qw = nn.Linear(dim, dim, bias = bias)\n",
    "        self.kw = nn.Linear(dim, dim, bias = bias)\n",
    "        self.vw = nn.Linear(dim, dim, bias = bias)\n",
    "\n",
    "        self.ow = nn.Linear(dim, dim, bias = bias)\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(maxlen, maxlen)).view(1, 1, maxlen, maxlen))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, L, D = x.shape\n",
    "        q = self.qw(x)\n",
    "        k = self.qw(x)\n",
    "        v = self.qw(x)\n",
    "\n",
    "        B, L, D = q.shape\n",
    "        q = torch.reshape(q, [B, L, self.n_heads, -1])\n",
    "        q = torch.permute(q, [0, 2, 1, 3])\n",
    "        k = torch.reshape(k, [B, L, self.n_heads, -1])\n",
    "        k = torch.permute(k, [0, 2, 3, 1])\n",
    "        v = torch.reshape(v, [B, L, self.n_heads, -1])\n",
    "        v = torch.permute(v, [0, 2, 1, 3])\n",
    "\n",
    "        qk = torch.matmul(q, k) * self.scale\n",
    "        qk = qk.masked_fill(self.bias[:,:,:L,:L] == 0, float('-inf'))\n",
    "        \n",
    "        attn = torch.softmax(qk, dim=-1)\n",
    "\n",
    "        v_attn = torch.matmul(attn, v)\n",
    "        v_attn = torch.permute(v_attn, [0, 2, 1, 3])\n",
    "        v_attn = torch.reshape(v_attn, [B, L, D])\n",
    "\n",
    "        x = self.ow(v_attn)\n",
    "        return x\n",
    "\n",
    "\n",
    "test_layer = Attention(32, maxlen, n_heads=1)\n",
    "test_layer(torch.ones([1, maxlen, 32]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Definir Transformer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rqDwbCJCAO0A",
    "outputId": "195c2c34-353d-40e9-e913-e2875c8333e8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 32])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, maxlen, heads=4, mlp_dim=512, rate=0.0):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(dim)\n",
    "        self.attn = Attention(dim, maxlen)\n",
    "        self.ln_2 = nn.LayerNorm(dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim, mlp_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(rate),\n",
    "            nn.Linear(mlp_dim, dim),\n",
    "            nn.Dropout(rate),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.attn(self.ln_1(x)) + x\n",
    "        return self.mlp(self.ln_2(x)) + x\n",
    "\n",
    "\n",
    "test_layer = Transformer(32, maxlen)\n",
    "test_layer(torch.ones([1, maxlen, 32])).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WskSwSw_A3g5",
    "outputId": "7a4b546a-b3c6-4451-be83-d5b06453b414"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 37])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pR1QIctMBNj5",
    "outputId": "b906948d-4c9a-41b1-ec20-658adbd41af6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    3,     6,     5,     4,  5207,   384,  5373,   110,   234,   157,\n",
       "          2234,    18,   454,   110,     2,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1],\n",
       "        [    3,     4,     5,     6,    40,  1059,    16,    84,  1087,  2160,\n",
       "             7,  4819,   219,   309,  5186,    11,    77,  1063,   906,  2158,\n",
       "            54, 19797,  1727,   311,  5185,    11,     2,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_batch[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Definir GPT y agregar embedding de posición:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MNOuhWgj_o8z",
    "outputId": "6586f440-b62a-4986-f356-6cac17fbdffe",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([128, 37, 37535]), torch.Size([128, 37]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, dim, vocab_size, maxlen, depth=3, \n",
    "                 mlp_dim=512, rate=0.2):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, dim)\n",
    "        self.pos_embedding = nn.Parameter(\n",
    "            torch.randn(1, maxlen, dim))\n",
    "\n",
    "        self.transformer = nn.Sequential()\n",
    "        for _ in range(depth):\n",
    "            self.transformer.append(Transformer(dim, maxlen))\n",
    "\n",
    "        self.head = nn.Linear(dim, vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, L = x.shape\n",
    "        x = self.embedding(x)\n",
    "        x += self.pos_embedding[:, :L]\n",
    "        x = self.transformer(x)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "model_dim = 128\n",
    "depth = 3\n",
    "mlp_dim = 128\n",
    "\n",
    "gpt = GPT(dim=model_dim, vocab_size=vocab_size, \n",
    "          maxlen=maxlen, depth=depth, mlp_dim=mlp_dim)\n",
    "output = gpt(train_batch)\n",
    "output.shape, target_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MsJbLyZ6b_57"
   },
   "source": [
    "## 4.- Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EvsuJHMBlm9y",
    "outputId": "ff57e59e-6632-40b4-d81c-085cbcc20a25"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (embedding): Embedding(37535, 128)\n",
       "  (transformer): Sequential(\n",
       "    (0): Transformer(\n",
       "      (ln_1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qw): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (kw): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (vw): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (ow): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln_2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Dropout(p=0.0, inplace=False)\n",
       "        (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (4): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (1): Transformer(\n",
       "      (ln_1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qw): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (kw): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (vw): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (ow): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln_2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Dropout(p=0.0, inplace=False)\n",
       "        (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (4): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (2): Transformer(\n",
       "      (ln_1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qw): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (kw): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (vw): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (ow): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln_2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Dropout(p=0.0, inplace=False)\n",
       "        (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (4): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (head): Linear(in_features=128, out_features=37535, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "gpt.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PAD_IDX = vocab.get_stoi()['<pad>']\n",
    "PAD_IDX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "wl9MQCp9TLCh"
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(gpt.parameters(), lr=0.001)\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "R5bmAldOcJn0"
   },
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    start = time.time()\n",
    "    running_loss = 0.0\n",
    "    model.train()\n",
    "    for inputs, targets in train_loader:\n",
    "        targets = targets.view(-1)\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        outputs = outputs.view(-1, outputs.size(-1))\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f'\\nTime for epoch {epoch} is {time.time()-start:4f} sec Train loss: {running_loss / len(train_loader):4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Traducción autorregresiva."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "translate spanish to english me encantan los perros arrendar doctorate tendrás aparcaron replace media acaloró accusing cancela scrub manifestado costos purchases hacéis defiende encendiste anotación accuse asustada dirá opposing ausenté comprometió califica descansara elementary pounds your dibujarme exploradores tortura techno promete boggle making despise ¿aceptan automáticos hike mejunje futuro folding despacho enchiladas charles llevaran smallest introduced\n",
      "\n",
      "translate spanish to english me gusta dormir rechacé dominación atrajo shaken televisores viewed downs extender klingon apreciadas hervir necesitó ex-girlfriends encantaba creando descended repetirlo mistakes legends professional shinano ocuparlo endebles enderezaran capacidades pensabas napoleon verb biblia trancado inquietos backpack conseguiré was lovingly drawn-out diseño clears monkeys service shelters decimos mower emborrachar cradle pasen tokio prados bomber stand\n",
      "\n",
      "translate english to spanish the cat is red pequeña mamífero mordisqueando ésta enamoraste mito rigidez bostezar avergonzado rowboat proponga tropezó llame adónde tardaron sobreviviste proteínas necesitar impedido vivió clonado agresiva contar capacidades pensabas napoleon verb biblia trancado restrooms superarás cholesterol beses iré sudaban atoró tomarse encueroles contraventanas trabajase chirrido amen tony documentos apagad tokio besaron aprovechan\n"
     ]
    }
   ],
   "source": [
    "def translate(model, sentence, device, maxlen):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        idx = torch.tensor([vocab[token] for token in tokenizer(sentence)],\n",
    "                                    dtype=torch.long)\n",
    "        idx = idx.reshape([1, -1])\n",
    "        maxlen = maxlen - idx.shape[-1]\n",
    "\n",
    "        for _ in range(maxlen):\n",
    "            idx = idx.to(device)\n",
    "            logits = gpt(idx)[:, -1, :]      \n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "\n",
    "            _, idx_next = torch.topk(probs, k=1, dim=-1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "        txt = \" \".join(\n",
    "                    [vocab.get_itos()[idx[0, _]] for _ in range(maxlen)]\n",
    "                )\n",
    "    return txt.replace(\"<eos>\", \"\")\n",
    "        \n",
    "sentences = ['translate spanish to english me encantan los perros',\n",
    "             'translate spanish to english me gusta dormir',\n",
    "             'translate english to spanish the cat is red']\n",
    "\n",
    "for s in sentences:\n",
    "    trans = translate(gpt, s, device, maxlen)\n",
    "    print(f\"\\n{trans}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "gtR1RoSBkARg",
    "outputId": "5e15136d-9e74-4734-d9d0-307ddbcd59d0",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time for epoch 0 is 29.692489 sec Train loss: 3.590706\n",
      "translate spanish to english me encantan los perros . i ' ve been waiting for you .                                       \n",
      "translate spanish to english me gusta dormir . i ' ll be able to the truth .                                        \n",
      "translate english to spanish the cat is red . el mundo está en la ciudad .                                        \n",
      "\n",
      "Time for epoch 1 is 29.499190 sec Train loss: 2.836563\n",
      "translate spanish to english me encantan los perros . i ' ll take the dishes .                                        \n",
      "translate spanish to english me gusta dormir . i like to see you .                                           \n",
      "translate english to spanish the cat is red . el mundo está vacío .                                          \n",
      "\n",
      "Time for epoch 2 is 30.072896 sec Train loss: 2.523122\n",
      "translate spanish to english me encantan los perros . i like the others .                                          \n",
      "translate spanish to english me gusta dormir . i like to sleep .                                            \n",
      "translate english to spanish the cat is red . el gato está roto .                                          \n",
      "\n",
      "Time for epoch 3 is 29.129706 sec Train loss: 2.290091\n",
      "translate spanish to english me encantan los perros . i love weddings .                                           \n",
      "translate spanish to english me gusta dormir . i like to sleep .                                            \n",
      "translate english to spanish the cat is red . el gato es rojo .                                          \n",
      "\n",
      "Time for epoch 4 is 30.378980 sec Train loss: 2.117090\n",
      "translate spanish to english me encantan los perros . i love cats .                                           \n",
      "translate spanish to english me gusta dormir . i like to sleep .                                            \n",
      "translate english to spanish the cat is red . el gato está mordiendo .                                          \n",
      "\n",
      "Time for epoch 5 is 29.227937 sec Train loss: 1.990856\n",
      "translate spanish to english me encantan los perros . i love dogs .                                           \n",
      "translate spanish to english me gusta dormir . i like to sleep .                                            \n",
      "translate english to spanish the cat is red . el gato es rojo .                                          \n"
     ]
    }
   ],
   "source": [
    "epochs = 6\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train(gpt, device, train_loader, optimizer, epoch)\n",
    "    \n",
    "    # Translate test sentences\n",
    "    for s in sentences:\n",
    "        trans = translate(gpt, s, device, maxlen)\n",
    "        print(trans)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
