{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The MIT License (MIT) Copyright (c) 2025 Emilio Morales\n",
    "#\n",
    "# Permission is hereby granted, free of charge, to any person obtaining a copy of \n",
    "# this software and associated documentation files (the \"Software\"), to deal in the Software without \n",
    "# restriction, including without limitation the rights to use, copy, modify, merge, publish, \n",
    "# distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the \n",
    "# Software is furnished to do so, subject to the following conditions:\n",
    "#\n",
    "# The above copyright notice and this permission notice shall be included in all copies or \n",
    "# substantial portions of the Software.\n",
    "#\n",
    "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, \n",
    "# INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND \n",
    "# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES \n",
    "# OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN \n",
    "# CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/milmor/NLP/blob/main/Notebooks/16_GPT.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />\n",
    "    Run in Google Colab</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ty7Z3RRm2v0P"
   },
   "source": [
    "# GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ryOL1_XG4ABQ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2.5.1+cu124', '3.6.0')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' # Disable tensorflow debugging logs\n",
    "os.environ[\"KERAS_BACKEND\"] = \"torch\"\n",
    "import keras\n",
    "import torch\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import random\n",
    "\n",
    "torch.__version__, keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ryOL1_XG4ABQ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7d4097e5b3d0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(77)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.- Conjunto de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text_file = keras.utils.get_file(\n",
    "    fname=\"spa-eng.zip\",\n",
    "    origin=\"http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\",\n",
    "    extract=True,\n",
    ")\n",
    "text_file = pathlib.Path(text_file).parent / \"spa-eng\" / \"spa.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "translate spanish to english: estudiar tres horas a la semana no será suficiente para aprender un idioma bien. studying three hours a week wouldn't be enough to learn a language well.\n",
      "translate spanish to english: hice que se enfadara la mujer. i made the woman angry.\n",
      "translate spanish to english: el sol está brillando intensamente. the sun is shining brightly.\n",
      "translate spanish to english: arrendamos una cabaña cerca de un lago. we rented a cabin by a lake.\n",
      "translate spanish to english: prefiero quedarme en casa que ir a pescar. i'd rather stay at home than go fishing.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "118964"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(text_file) as f:\n",
    "    lines = f.read().split(\"\\n\")[:-1]\n",
    "    \n",
    "text_pairs = []\n",
    "\n",
    "for line in lines:\n",
    "    eng, spa = line.split(\"\\t\")\n",
    "    text_pairs.append('translate spanish to english: ' + spa.lower() + ' ' + eng.lower())\n",
    "\n",
    "for _ in range(5):\n",
    "    print(random.choice(text_pairs))\n",
    "\n",
    "len(text_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "translate spanish to english: tom le preguntó a mary acerca de john. tom asked mary about john.\n",
      "translate spanish to english: tom pagó con tarjeta de crédito. tom paid by credit card.\n",
      "translate spanish to english: es nuestra única oportunidad. this is our only shot.\n",
      "translate spanish to english: tuve que ir a casa a cambiarme. i had to go home and change.\n",
      "translate spanish to english: sé todo eso. i know all that.\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    print(random.choice(text_pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Conjuntos de entrenamiento, prueba y validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "_LIiVqBco41j"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118964 total pairs\n",
      "118370 training pairs\n",
      "594 test pairs\n"
     ]
    }
   ],
   "source": [
    "random.Random(77).shuffle(text_pairs)\n",
    "num_val_samples = int(0.005 * len(text_pairs))\n",
    "num_train_samples = len(text_pairs) - num_val_samples\n",
    "train_pairs = text_pairs[:num_train_samples]\n",
    "test_pairs = text_pairs[num_train_samples:]\n",
    "\n",
    "print(f\"{len(text_pairs)} total pairs\")\n",
    "print(f\"{len(train_pairs)} training pairs\")\n",
    "print(f\"{len(test_pairs)} test pairs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mN_d_pESTAkZ"
   },
   "source": [
    "## 2.- Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "cW6E9jkJTe1o"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs: [3732, 24, 3818]\n",
      "Decoded Text: hello , hola\n",
      "Vocabulary size: 34000\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "import os\n",
    "\n",
    "# Set environment variable to avoid tokenizers parallelism warning\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "trainer = WordLevelTrainer(special_tokens=[\"[UNK]\", \"[PAD]\", \"[EOS]\"], \n",
    "                           min_frequency=1, vocab_size=34000)\n",
    "\n",
    "tokenizer.train_from_iterator(train_pairs, trainer=trainer)\n",
    "\n",
    "text = \"hello, hola\"\n",
    "encoding = tokenizer.encode(text)\n",
    "print(\"Token IDs:\", encoding.ids)\n",
    "decoded_text = tokenizer.decode(encoding.ids)\n",
    "print(\"Decoded Text:\", decoded_text)\n",
    "vocab_size = tokenizer.get_vocab_size()\n",
    "print(f\"Vocabulary size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 64]) torch.Size([128, 64])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len + 1  # for EOS\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        input_ids = self.tokenizer.encode(text).ids[:self.max_len - 1]  # Reserve space for EOS\n",
    "        return torch.tensor(input_ids, dtype=torch.long)\n",
    "\n",
    "# Collate with EOS + padding\n",
    "def collate_fn(batch):\n",
    "    # Append EOS to each sequence\n",
    "    eos = torch.tensor([EOS_IDX], dtype=torch.long)\n",
    "    batch = [torch.cat([seq, eos]) for seq in batch]\n",
    "\n",
    "    # Create input/output pairs: x ([:-1]), y ([1:])\n",
    "    x = [seq[:-1] for seq in batch]\n",
    "    y = [seq[1:] for seq in batch]\n",
    "\n",
    "    # Pad both\n",
    "    x_padded = pad_sequence(x, batch_first=True, padding_value=PAD_IDX)\n",
    "    y_padded = pad_sequence(y, batch_first=True, padding_value=PAD_IDX)\n",
    "    return x_padded, y_padded\n",
    "\n",
    "# Setup\n",
    "PAD_IDX = tokenizer.token_to_id(\"[PAD]\") \n",
    "EOS_IDX = tokenizer.token_to_id(\"[EOS]\") \n",
    "maxlen = 64\n",
    "batch_size = 128\n",
    "\n",
    "train_dataset = TextDataset(train_pairs, tokenizer, max_len=maxlen)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n",
    "                          num_workers=4, pin_memory=True, collate_fn=collate_fn)\n",
    "\n",
    "# Check batch\n",
    "train_batch, train_label = next(iter(train_loader))\n",
    "print(train_batch.shape, train_label.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u_N6IRYsT4ry",
    "outputId": "a1d1522d-c8db-493b-fc77-e3549aeeb40a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68.2 ms ± 546 μs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "train_batch, target_batch = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "aauQX2tFXD8K"
   },
   "outputs": [],
   "source": [
    "train_batch, target_batch = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ROw_9EjsT2Et",
    "outputId": "37555466-8faa-49bf-898e-e7374d2c66db"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([128, 33]), torch.Size([128, 33]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_batch.shape, target_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pR1QIctMBNj5",
    "outputId": "b906948d-4c9a-41b1-ec20-658adbd41af6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    8,     7,     4,     5,     6,    20,   197,  2870,    16,    18,\n",
       "          7999,    53,    76,   597,    16,   851,    18,    10, 10961,    13,\n",
       "            56,    15,  2031,    34,  7999,    23,  4379,     4,   851,   170,\n",
       "             4, 10961,    13],\n",
       "        [    8,     7,     4,     5,     6,    60,    59,   203,    18,  2140,\n",
       "            26,    19, 18561,     3,    47,   228,    80,    34,    47,  2609,\n",
       "            32, 18560,     3,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_batch[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    7,     4,     5,     6,    20,   197,  2870,    16,    18,  7999,\n",
       "            53,    76,   597,    16,   851,    18,    10, 10961,    13,    56,\n",
       "            15,  2031,    34,  7999,    23,  4379,     4,   851,   170,     4,\n",
       "         10961,    13,     2],\n",
       "        [    7,     4,     5,     6,    60,    59,   203,    18,  2140,    26,\n",
       "            19, 18561,     3,    47,   228,    80,    34,    47,  2609,    32,\n",
       "         18560,     3,     2,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_batch[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[EOS]'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.id_to_token(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PZ7f4DHJreIj"
   },
   "source": [
    "## 3.- Modelo\n",
    "- Definir auto atención producto punto con máscara:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mbox{MultiHead}(Q, K, V) = \\text{Concat}(\\mbox{head}_1,\\mbox{head}_2,\\ldots,\\mbox{head}_h)W^O,\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\mbox{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V) = \\text{softmax}\\left[\\frac{QW_i^Q(KW_i^K)^T}{\\sqrt{d_k}}\\right]VW_i^V,\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dT6uffV0rdh_",
    "outputId": "1f783aa2-47b4-4c1f-b243-e512ee31a3ed",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.4182, -0.4156,  0.6756,  ...,  0.0086, -0.0181, -0.3255],\n",
       "         [ 0.4182, -0.4156,  0.6756,  ...,  0.0086, -0.0181, -0.3255],\n",
       "         [ 0.4182, -0.4156,  0.6756,  ...,  0.0086, -0.0181, -0.3255],\n",
       "         ...,\n",
       "         [ 0.4182, -0.4156,  0.6756,  ...,  0.0086, -0.0181, -0.3255],\n",
       "         [ 0.4182, -0.4156,  0.6756,  ...,  0.0086, -0.0181, -0.3255],\n",
       "         [ 0.4182, -0.4156,  0.6756,  ...,  0.0086, -0.0181, -0.3255]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, maxlen, n_heads=4, bias=True):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.scale = (dim // n_heads) ** -0.5\n",
    "        self.qw = nn.Linear(dim, dim, bias = bias)\n",
    "        self.kw = nn.Linear(dim, dim, bias = bias)\n",
    "        self.vw = nn.Linear(dim, dim, bias = bias)\n",
    "\n",
    "        self.ow = nn.Linear(dim, dim, bias = bias)\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(maxlen, maxlen)).view(1, 1, maxlen, maxlen))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, L, D = x.shape\n",
    "        q = self.qw(x)\n",
    "        k = self.kw(x)\n",
    "        v = self.vw(x)\n",
    "\n",
    "        B, L, D = q.shape\n",
    "        q = torch.reshape(q, [B, L, self.n_heads, -1])\n",
    "        q = torch.permute(q, [0, 2, 1, 3])\n",
    "        k = torch.reshape(k, [B, L, self.n_heads, -1])\n",
    "        k = torch.permute(k, [0, 2, 3, 1])\n",
    "        v = torch.reshape(v, [B, L, self.n_heads, -1])\n",
    "        v = torch.permute(v, [0, 2, 1, 3])\n",
    "\n",
    "        qk = torch.matmul(q, k) * self.scale\n",
    "        qk = qk.masked_fill(self.bias[:,:,:L,:L] == 0, float('-inf'))\n",
    "        \n",
    "        attn = torch.softmax(qk, dim=-1)\n",
    "\n",
    "        v_attn = torch.matmul(attn, v)\n",
    "        v_attn = torch.permute(v_attn, [0, 2, 1, 3])\n",
    "        v_attn = torch.reshape(v_attn, [B, L, D])\n",
    "\n",
    "        x = self.ow(v_attn)\n",
    "        return x\n",
    "\n",
    "\n",
    "test_layer = Attention(32, maxlen, n_heads=1)\n",
    "test_layer(torch.ones([1, maxlen, 32]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Definir Transformer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rqDwbCJCAO0A",
    "outputId": "195c2c34-353d-40e9-e913-e2875c8333e8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 32])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, maxlen, heads=4, mlp_dim=512, rate=0.0):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(dim)\n",
    "        self.attn = Attention(dim, maxlen)\n",
    "        self.ln_2 = nn.LayerNorm(dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim, mlp_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(rate),\n",
    "            nn.Linear(mlp_dim, dim),\n",
    "            nn.Dropout(rate),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.attn(self.ln_1(x)) + x\n",
    "        return self.mlp(self.ln_2(x)) + x\n",
    "\n",
    "\n",
    "test_layer = Transformer(32, maxlen)\n",
    "test_layer(torch.ones([1, maxlen, 32])).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WskSwSw_A3g5",
    "outputId": "7a4b546a-b3c6-4451-be83-d5b06453b414"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 33])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Definir GPT y agregar embedding de posición:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MNOuhWgj_o8z",
    "outputId": "6586f440-b62a-4986-f356-6cac17fbdffe",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([128, 33, 34000]), torch.Size([128, 33]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, dim, vocab_size, maxlen, depth=3, \n",
    "                 mlp_dim=512, rate=0.2):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, dim)\n",
    "        self.pos_embedding = nn.Parameter(\n",
    "            torch.randn(1, maxlen, dim))\n",
    "\n",
    "        self.transformer = nn.Sequential()\n",
    "        for _ in range(depth):\n",
    "            self.transformer.append(Transformer(dim, maxlen))\n",
    "\n",
    "        self.head = nn.Linear(dim, vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, L = x.shape\n",
    "        x = self.embedding(x)\n",
    "        x += self.pos_embedding[:, :L]\n",
    "        x = self.transformer(x)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "model_dim = 128\n",
    "depth = 3\n",
    "mlp_dim = 128\n",
    "\n",
    "gpt = GPT(dim=model_dim, vocab_size=vocab_size, \n",
    "          maxlen=maxlen, depth=depth, mlp_dim=mlp_dim)\n",
    "output = gpt(train_batch)\n",
    "output.shape, target_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MsJbLyZ6b_57"
   },
   "source": [
    "## 4.- Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EvsuJHMBlm9y",
    "outputId": "ff57e59e-6632-40b4-d81c-085cbcc20a25"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (embedding): Embedding(34000, 128)\n",
       "  (transformer): Sequential(\n",
       "    (0): Transformer(\n",
       "      (ln_1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qw): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (kw): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (vw): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (ow): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln_2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Dropout(p=0.0, inplace=False)\n",
       "        (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (4): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (1): Transformer(\n",
       "      (ln_1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qw): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (kw): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (vw): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (ow): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln_2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Dropout(p=0.0, inplace=False)\n",
       "        (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (4): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (2): Transformer(\n",
       "      (ln_1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qw): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (kw): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (vw): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (ow): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln_2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Dropout(p=0.0, inplace=False)\n",
       "        (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (4): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (head): Linear(in_features=128, out_features=34000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "gpt.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "wl9MQCp9TLCh"
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(gpt.parameters(), lr=0.001)\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "R5bmAldOcJn0"
   },
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    start = time.time()\n",
    "    running_loss = 0.0\n",
    "    model.train()\n",
    "    for inputs, targets in train_loader:\n",
    "        targets = targets.view(-1)\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        outputs = outputs.view(-1, outputs.size(-1))\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f'\\nTime for epoch {epoch} is {time.time()-start:4f} sec Train loss: {running_loss / len(train_loader):4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Traducción autorregresiva."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "translate spanish to english él ha ido a jugar . aprendieras sospecho alleys permanecieron motivos buying doc jugó contarle amarilla rendiría cenas pegados arrepentirá punctual fundador creerían careless animal cuchilla coast móvil interpretar ropas blood comprarse desatarse bighorn special limpie staircase deprisa leerlo enferme literal , remaining 1650 incomplete ajustados apartarte desafíes following comprendidas introduje sincere ford cd vigila fricciones aclara clandestinas maybe italia\n",
      "\n",
      "translate spanish to english me gusta dormir . granjero repaired llamaré parecías feed auténtico semillas atarantado acantilado cuadras applicants snack conduzca malévolo ojeras guy elegirle fósforo belongings siguió babuinos childlike cancelled jubila paranoicos chicas desconfiado animados erguido enojarme listas peinar consultation outing mesita momentáneamente insegura clavel acreditado raisins partirá bolacero desamparada frost construida dirijo implicaciones pesar diente odiáis confirmaron enfrente honesta dreams preocuparnos slammed\n",
      "\n",
      "translate spanish to english él quiere ayudar . granjero repaired young dirijo urls pregunté encontraste atarantado acantilado cuadras applicants snack conduzca malévolo ojeras guy elegirle fósforo marcadas investigue burlé enterarnos simpáticos actualización oídos definitivo helped cuidando desiertos compuestos standards encontrame acuerde weekends lagos late indisciplinado staying dividieron dañinos building mitad engordan active linterna theaters apaciguaron lanzas enseguida contara merodeadores extendido rica boards eliminate cristiano\n"
     ]
    }
   ],
   "source": [
    "def translate(model, sentence, device, maxlen):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        encoding = tokenizer.encode(sentence)\n",
    "        idx =  torch.tensor(encoding.ids, dtype=torch.long)\n",
    "        idx = idx.reshape([1, -1])\n",
    "        maxlen = maxlen - idx.shape[-1]\n",
    "\n",
    "        for _ in range(maxlen):\n",
    "            idx = idx.to(device)\n",
    "            logits = gpt(idx)[:, -1, :]      \n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "\n",
    "            _, idx_next = torch.topk(probs, k=1, dim=-1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "        txt = \" \".join([tokenizer.id_to_token(_) for _ in idx[0]])\n",
    "    return txt.split(\"[EOS]\")[0].strip()\n",
    "        \n",
    "sentences = ['translate spanish to english él ha ido a jugar .',\n",
    "             'translate spanish to english me gusta dormir .',\n",
    "             'translate spanish to english él quiere ayudar .']\n",
    "\n",
    "for s in sentences:\n",
    "    trans = translate(gpt, s, device, maxlen)\n",
    "    print(f\"\\n{trans}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "gtR1RoSBkARg",
    "outputId": "5e15136d-9e74-4734-d9d0-307ddbcd59d0",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time for epoch 0 is 19.352762 sec Train loss: 3.433056\n",
      "translate spanish to english él ha ido a jugar . he has to go to sleep .\n",
      "translate spanish to english me gusta dormir . i ' ll be able to sleep .\n",
      "translate spanish to english él quiere ayudar . i wants to help him .\n",
      "\n",
      "Time for epoch 1 is 19.302342 sec Train loss: 2.470447\n",
      "translate spanish to english él ha ido a jugar . he went to play .\n",
      "translate spanish to english me gusta dormir . i like sleeping .\n",
      "translate spanish to english él quiere ayudar . i wants to help .\n",
      "\n",
      "Time for epoch 2 is 19.356095 sec Train loss: 2.128413\n",
      "translate spanish to english él ha ido a jugar . he went play .\n",
      "translate spanish to english me gusta dormir . i like sleeping .\n",
      "translate spanish to english él quiere ayudar . he wants to help .\n",
      "\n",
      "Time for epoch 3 is 19.485735 sec Train loss: 1.934638\n",
      "translate spanish to english él ha ido a jugar . he went to play .\n",
      "translate spanish to english me gusta dormir . i like sleeping .\n",
      "translate spanish to english él quiere ayudar . he wants to help .\n",
      "\n",
      "Time for epoch 4 is 19.492206 sec Train loss: 1.801779\n",
      "translate spanish to english él ha ido a jugar . he went on singing .\n",
      "translate spanish to english me gusta dormir . i like sleeping .\n",
      "translate spanish to english él quiere ayudar . he wants to help .\n",
      "\n",
      "Time for epoch 5 is 19.510119 sec Train loss: 1.703502\n",
      "translate spanish to english él ha ido a jugar . he has gone to play .\n",
      "translate spanish to english me gusta dormir . i like sleeping .\n",
      "translate spanish to english él quiere ayudar . he wants to help .\n"
     ]
    }
   ],
   "source": [
    "epochs = 6\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train(gpt, device, train_loader, optimizer, epoch)\n",
    "    \n",
    "    # Translate test sentences\n",
    "    for s in sentences:\n",
    "        trans = translate(gpt, s, device, maxlen)\n",
    "        print(trans)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
