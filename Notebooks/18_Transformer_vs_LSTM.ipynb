{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbc49d9b-5f41-4169-bbda-5d77f285459f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' # Disable tensorflow debugging logs\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953c4517-86bd-4ed3-836a-f7356c895317",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Importar dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3404f6ab-2dd4-4c7e-989a-ea33ee8c14f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "342afe6a-fe93-4832-b573-96f8e209cc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tfds.load('imdb_reviews', as_supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b155f67f-344e-4841-adab-3a2e58608756",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train_ds, raw_test_ds = dataset['train'], dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76dae875-74f1-46d0-a402-03cf4bdd4681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\" 0\n"
     ]
    }
   ],
   "source": [
    "for text, label in raw_train_ds.take(1):\n",
    "    print(text.numpy(), label.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b56f4b-0c53-466f-a810-48091062b5af",
   "metadata": {},
   "source": [
    "## Preparar dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39b8f2a7-f119-48ae-8dba-f8fa7fb03b22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "BUFFER_SIZE = tf.data.experimental.cardinality(raw_train_ds)\n",
    "BUFFER_SIZE.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b380bb5a-b845-4440-a468-147af7bbcb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "voc_size = 5000\n",
    "\n",
    "train_ds = raw_train_ds.shuffle(BUFFER_SIZE).batch(\n",
    "        batch_size, num_parallel_calls=AUTOTUNE).prefetch(\n",
    "        AUTOTUNE)\n",
    "\n",
    "test_ds = raw_test_ds.batch(\n",
    "        batch_size, num_parallel_calls=AUTOTUNE).prefetch(\n",
    "        AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48972e5d-a421-415c-bbae-c3b1440ea599",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'As a big fan of David Mamet\\'s films and plays, especially his first film House of Games that also starred Joe Mantegna, I was expecting great things from this film. Instead, I found myself annoyed by the film\\'s superficiality and lack of credibility. Racial slurs are thrown about without any feeling or meaning behind them, in the hopes of setting up a racial tension that for me never materialized. Identity is totally reevaluated and men become \"heroes\" for no apparent reason. Because of his oaths taken as a cop, the lead character adamantly refuses to perform one relatively small action that would harm no one and could possibly save lives, and yet performs another action which is very violent and VERY illegal, but then still refuses the minor action. In addition, a highly unbelievable subplot involving a man who has killed his family is introduced just for the sake of a plot point that was all but advertised with skywriting, and the cop\\'s reaction to that occurrence stretch credulity way beyond all reasonable limits. Needless to say, after expecting another exciting thriller from David Mamet, I was extremely disappointed to say the least. 3 out of 10.', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "for text, label in train_ds.take(1):\n",
    "    print(text[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bae1da-6e63-4d24-828d-3347905e8e10",
   "metadata": {},
   "source": [
    "## Tokenización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "288fb5a9-8fac-40a7-a696-d58adfa6d4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 128\n",
    "vectorize_layer = layers.TextVectorization(\n",
    "    max_tokens=voc_size,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ee458d-4856-40c4-b3f7-e9adeda67af5",
   "metadata": {},
   "source": [
    "- Adaptar la capa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42deec01-632e-406a-96d2-bea5dc563572",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize_layer_ds = train_ds.map(lambda text, label: text)\n",
    "vectorize_layer.adapt(vectorize_layer_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a992d26a-a2a5-43a6-801a-69185baa09da",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '[UNK]', 'the', 'and', 'a', 'of', 'to', 'is', 'in', 'it']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorize_layer.get_vocabulary()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd289ef-b4ea-43f9-b059-4439e4a71888",
   "metadata": {},
   "source": [
    "- Probar vectorize_layer con batch de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a5741fa-1582-4479-9697-9aa424da11be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 128), dtype=int64, numpy=\n",
       "array([[ 1, 48,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_batch = tf.constant([['Hi there']])\n",
    "vectorize_layer(test_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea078c7-a54d-40d5-ad6c-57a8e79cc616",
   "metadata": {},
   "source": [
    "## Definir LSTM\n",
    "\n",
    "RNN:\n",
    "\\begin{equation}\n",
    "h_t = f(Wx_t + Uh_{t-1} + b)\n",
    "\\end{equation}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210f06d3-2cb7-491e-bb56-30779cdc7e09",
   "metadata": {},
   "source": [
    "LSTM:\n",
    "\n",
    "\\begin{align}\n",
    "i_t & = \\sigma(W^ix_t + U^ih_{t-1} + b^i) \\\\\n",
    "f_t & = \\sigma(W^fx_t + U^fh_{t-1} + b^f) \\\\\n",
    "o_t & = \\sigma(W^ox_t + U^oh_{t-1} + b^o) \\\\\n",
    "g_t & = \\text{tanh}(W^gx_t + U^gh_{t-1} + b^g) \\\\\n",
    "c_t & = f_t \\odot c_{t-1} + i_t \\odot g_t\\\\\n",
    "h_t & = o_t \\odot \\text{tanh}(c_t) \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "59d56968-9a5f-4dec-adaf-a4e7d8f66f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = tf.keras.Sequential([\n",
    "    vectorize_layer,\n",
    "    layers.Embedding(\n",
    "        input_dim=voc_size, output_dim=128),\n",
    "    layers.Bidirectional(layers.LSTM(128)),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea47915-00bb-42f5-81e4-d33d3e37b4be",
   "metadata": {},
   "source": [
    "- Probar LSTM con batch de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aa876b8a-c6fa-4c9d-8819-476b70288bc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[-0.01765357]], dtype=float32)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm(test_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfca56a-eb6d-418c-9b11-4a5300ab9bd1",
   "metadata": {},
   "source": [
    "- Información del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a315c30f-f107-43fa-b5d6-1a4334ff0c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_vectorization (TextVec  (None, 128)              0         \n",
      " torization)                                                     \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 128, 128)          640000    \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 256)              263168    \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                16448     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 919,681\n",
      "Trainable params: 919,681\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lstm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78948806-9b9b-4a4b-960b-e3668b750240",
   "metadata": {},
   "source": [
    "## Entrenamiento LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "29a2b23f-a780-4b94-b280-5feeecc07ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1f299f93-f2ac-4964-a2fb-bdfd7668217e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_opt = tf.keras.optimizers.Adam(learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6578dcd0-2eca-4a94-8d6d-2efbba652d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_avg = tf.keras.metrics.Mean(name='train_loss')\n",
    "val_loss_avg = tf.keras.metrics.Mean(name='val_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a761811b-6555-43e6-ae7a-031c50781aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "298716de-4ec4-4b9a-a382-c6ffddc7335b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(text, target):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = lstm(text, training=True)\n",
    "        loss_value = loss(tf.cast(target, tf.float32), logits)\n",
    "\n",
    "    gradients = tape.gradient(loss_value, lstm.trainable_weights)\n",
    "    lstm_opt.apply_gradients(zip(gradients, lstm.trainable_weights))\n",
    "    train_loss_avg(loss_value)\n",
    "    \n",
    "@tf.function\n",
    "def test_step(text, target):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = lstm(text, training=False)\n",
    "        loss_value = loss(tf.cast(target, tf.float32), logits)\n",
    "\n",
    "    val_loss_avg(loss_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8d99446c-6232-47e6-bfee-38cedf9d9389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[1 0 1 0 1 0 1 0 1 1 0 1 1 1 1 0 0 1 0 0 0 1 0 1 1 0 1 0 1 1 0 1 0 0 0 1 1\n",
      " 1 1 0 1 1 0 0 1 0 0 0 1 1 0 1 1 1 1 1 0 1 0 1 0 0 1 0 1 0 1 1 0 1 0 1 1 1\n",
      " 1 1 0 0 1 0 1 1 1 1 1 1 0 1 1 0 0 1 0 1 1 1 1 1 0 1 1 0 0 0 1 1 0 1 1 0 1\n",
      " 0 1 0 0 0 0 0 1 0 1 0 0 1 1 1 1 0], shape=(128,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for text, target in train_ds.take(1):\n",
    "    print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d8d7010c-48b3-49e3-bf55-16c19197b65c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Train loss: 0.6584\n",
      "Val loss: 0.5253\n",
      "Epoch: 1 Train loss: 0.4119\n",
      "Val loss: 0.3884\n",
      "Epoch: 2 Train loss: 0.3289\n",
      "Val loss: 0.3850\n",
      "Epoch: 3 Train loss: 0.2939\n",
      "Val loss: 0.3870\n",
      "Epoch: 4 Train loss: 0.2763\n",
      "Val loss: 0.3816\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    for text, target in train_ds:\n",
    "        train_step(text, target)\n",
    "        \n",
    "    print(f'Epoch: {epoch} Train loss: {train_loss_avg.result():.4f}')\n",
    "    train_loss_avg.reset_states()\n",
    "    \n",
    "    for text, target in test_ds:\n",
    "        test_step(text, target)\n",
    "        \n",
    "    print(f'Val loss: {val_loss_avg.result():.4f}')\n",
    "    val_loss_avg.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0022f0-1275-4fae-9339-efc3e2001bd1",
   "metadata": {},
   "source": [
    "## Definir Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5fec4fb-fc1f-43c1-adb2-f0bd9e93be19",
   "metadata": {},
   "source": [
    "<img src=\"../img/dot_product.png\" width=\"500\"/>\n",
    "\n",
    "__Imagen tomada de Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). Attention is all you need. Advances in neural information processing systems, 30.__\n",
    "\n",
    "\\begin{equation}\n",
    "\\mbox{MultiHead}(Q, K, V) = \\text{Concat}(\\mbox{head}_1,\\mbox{head}_2,\\ldots,\\mbox{head}_h)W^O,\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\mbox{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V) = \\text{softmax}\\left[\\frac{QW_i^Q(KW_i^K)^T}{\\sqrt{d_k}}\\right]VW_i^V,\n",
    "\\label{eq:selfattention}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6354d4e4-52cd-45d2-bc30-7738b52a30f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(layers.Layer):\n",
    "    def __init__(self, model_dim, n_heads, rate=0.1, initializer='glorot_uniform'):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.model_dim = model_dim\n",
    "\n",
    "        assert model_dim % self.n_heads == 0\n",
    "\n",
    "        self.head_dim = model_dim // self.n_heads\n",
    "\n",
    "        self.wq = layers.Dense(model_dim, kernel_initializer=initializer)\n",
    "        self.wk = layers.Dense(model_dim, kernel_initializer=initializer)\n",
    "        self.wv = layers.Dense(model_dim, kernel_initializer=initializer)\n",
    "        \n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "        \n",
    "        self.wo = layers.Dense(model_dim, kernel_initializer=initializer)\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.n_heads, self.head_dim))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, q, k, v):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "\n",
    "        q = self.wq(q)  \n",
    "        k = self.wk(k)  \n",
    "        v = self.wv(v)  \n",
    "\n",
    "        q = self.split_heads(q, batch_size) \n",
    "        k = self.split_heads(k, batch_size)  \n",
    "        v = self.split_heads(v, batch_size) \n",
    "\n",
    "        dh = tf.cast(self.head_dim, tf.float32)\n",
    "        qk = tf.matmul(q, k, transpose_b=True)\n",
    "        scaled_qk =  qk / tf.math.sqrt(dh)\n",
    "\n",
    "        attn = self.dropout1(tf.nn.softmax(scaled_qk, axis=-1))\n",
    "        attn = tf.matmul(attn, v) \n",
    "\n",
    "        attn = tf.transpose(attn, perm=[0, 2, 1, 3]) \n",
    "        original_size_attention = tf.reshape(attn, (batch_size, -1, self.model_dim)) \n",
    "\n",
    "        output = self.dropout2(self.wo(original_size_attention))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4619cb0d-3f8c-49d1-957b-a8a41a59a952",
   "metadata": {},
   "source": [
    "- Definir embedding de posición"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "432249e4-1667-41d6-bb0b-9f218f02dfd1",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TokenEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, emb_dim, \n",
    "                 rate=0.0):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.max_len = maxlen\n",
    "        self.token_emb = layers.Embedding(\n",
    "            input_dim=vocab_size, output_dim=emb_dim)\n",
    "        self.position_emb = layers.Embedding(\n",
    "            input_dim=maxlen, output_dim=emb_dim)\n",
    "        self.dropout = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x):\n",
    "        token_embeddings = self.token_emb(x)\n",
    "        positions = tf.range(start=0, limit=self.max_len, delta=1)\n",
    "        positions = self.position_emb(positions)\n",
    "        return self.dropout(token_embeddings + positions) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7d7859bf-5847-41b0-92a1-aabca46476a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, model_dim, n_heads=2, mlp_dim=512, \n",
    "                 rate=0.0, eps=1e-6):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attn = MultiHeadAttention(model_dim, n_heads)\n",
    "        self.mlp = tf.keras.Sequential([\n",
    "            layers.Dense(mlp_dim, activation='gelu'), \n",
    "            layers.Dense(model_dim),\n",
    "            layers.Dropout(rate)\n",
    "        ])\n",
    "        self.ln1 = layers.LayerNormalization(epsilon=eps)\n",
    "        self.ln2 = layers.LayerNormalization(epsilon=eps)\n",
    "        self.drop1 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):  \n",
    "        x = self.drop1(self.attn(inputs, inputs, inputs), training=training) \n",
    "        x = self.ln1(x + inputs)\n",
    "        return self.ln2(self.mlp(x) + x)\n",
    "    \n",
    "block = TransformerBlock(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "673129bd-0046-4da4-98a6-d3f6d43f9f92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.45826337]], dtype=float32)>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Transformer(tf.keras.models.Model):\n",
    "    def __init__(self, model_dim, voc_size, mlp_dim=256, \n",
    "                 maxlen=128, heads=4):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.emb = TokenEmbedding(maxlen, voc_size, model_dim)\n",
    "        self.block = TransformerBlock(model_dim, heads, mlp_dim)\n",
    "        self.out = tf.keras.Sequential([\n",
    "            layers.GlobalAveragePooling1D(),\n",
    "            layers.Dense(1)\n",
    "        ])\n",
    "    \n",
    "    def call(self, x):\n",
    "        x = vectorize_layer(x)\n",
    "        x = self.emb(x)\n",
    "        x = self.block(x)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "    \n",
    "transformer = Transformer(128, voc_size)\n",
    "transformer(test_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e426a166-bacb-4e22-a2f5-b6208621308c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " token_embedding (TokenEmbed  multiple                 656384    \n",
      " ding)                                                           \n",
      "                                                                 \n",
      " transformer_block_1 (Transf  multiple                 132480    \n",
      " ormerBlock)                                                     \n",
      "                                                                 \n",
      " sequential_3 (Sequential)   (1, 1)                    129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 788,993\n",
      "Trainable params: 788,993\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "transformer.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a1358c-38e6-491b-8489-cfb9e1de8b52",
   "metadata": {},
   "source": [
    "## Entrenamiento Transformer\n",
    "- Utilizar los mismos parámteros de LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "370c0112-887f-4c32-b364-04c05ae68195",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ba85bf54-db2b-46dc-b4bc-bf599b2c4677",
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_opt = tf.keras.optimizers.Adam(learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "05f86685-9fba-4c5e-8906-d3a5f43877f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_avg = tf.keras.metrics.Mean(name='train_loss')\n",
    "val_loss_avg = tf.keras.metrics.Mean(name='val_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "56b431b6-dc3a-4245-8060-d41898c61bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5f2ac08c-0a6b-48d1-89d5-be0b6c3933b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(text, target):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = transformer(text, training=True)\n",
    "        loss_value = loss(tf.cast(target, tf.float32), logits)\n",
    "\n",
    "    gradients = tape.gradient(loss_value, transformer.trainable_weights)\n",
    "    trans_opt.apply_gradients(zip(gradients, transformer.trainable_weights))\n",
    "    train_loss_avg(loss_value)\n",
    "    \n",
    "@tf.function\n",
    "def test_step(text, target):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = transformer(text, training=False)\n",
    "        loss_value = loss(tf.cast(target, tf.float32), logits)\n",
    "\n",
    "    val_loss_avg(loss_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7d2a6269-eee4-408a-b358-654c8078efd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Train loss: 0.5711\n",
      "Val loss: 0.4511\n",
      "Epoch: 1 Train loss: 0.3669\n",
      "Val loss: 0.3730\n",
      "Epoch: 2 Train loss: 0.3081\n",
      "Val loss: 0.3729\n",
      "Epoch: 3 Train loss: 0.2848\n",
      "Val loss: 0.3890\n",
      "Epoch: 4 Train loss: 0.2646\n",
      "Val loss: 0.3964\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    for text, target in train_ds:\n",
    "        train_step(text, target)\n",
    "        \n",
    "    print(f'Epoch: {epoch} Train loss: {train_loss_avg.result():.4f}')\n",
    "    train_loss_avg.reset_states()\n",
    "    \n",
    "    for text, target in test_ds:\n",
    "        test_step(text, target)\n",
    "        \n",
    "    print(f'Val loss: {val_loss_avg.result():.4f}')\n",
    "    val_loss_avg.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c48752-cf76-48ac-997d-8f245d257811",
   "metadata": {},
   "source": [
    "## Ejercicio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1df65d-ba82-4850-b59b-768a3cc8ff51",
   "metadata": {},
   "source": [
    "- Modificar los hiperparámetros de los modelos para obtener mejores resultados.\n",
    "- Modificar las arquitecturas, comparar resultados con GRU.\n",
    "- Agregar y modificar regularización."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
