{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The MIT License (MIT) Copyright (c) 2025 Emilio Morales\n",
    "#\n",
    "# Permission is hereby granted, free of charge, to any person obtaining a copy of \n",
    "# this software and associated documentation files (the \"Software\"), to deal in the Software without \n",
    "# restriction, including without limitation the rights to use, copy, modify, merge, publish, \n",
    "# distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the \n",
    "# Software is furnished to do so, subject to the following conditions:\n",
    "#\n",
    "# The above copyright notice and this permission notice shall be included in all copies or \n",
    "# substantial portions of the Software.\n",
    "#\n",
    "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, \n",
    "# INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND \n",
    "# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES \n",
    "# OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN \n",
    "# CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/milmor/NLP/blob/main/Notebooks/13_Text_generation_RNN_HP.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />\n",
    "    Run in Google Colab</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ty7Z3RRm2v0P"
   },
   "source": [
    "# Generación de texto\n",
    "  \n",
    "- Harry Potter book: https://www.kaggle.com/datasets/shubhammaindola/harry-potter-books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.5.1+cu124'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ryOL1_XG4ABQ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x74396ff1f190>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(77)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.- Conjuntos de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words: 439478\n"
     ]
    }
   ],
   "source": [
    "path = './01 Harry Potter and the Sorcerers Stone.txt'\n",
    "book = open(path, 'rb').read().decode(encoding='utf-8').lower()\n",
    "\n",
    "print(f'Words: {len(book)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "words = re.findall(r'\\b\\w+\\b|[\\.,;!?()\"\\']', book)\n",
    "\n",
    "maxlen = 50\n",
    "# Crear lotes de 50 palabras\n",
    "sentences = [words[i:i + maxlen] for i in range(0, len(words), maxlen)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['m',\n",
       " 'r',\n",
       " '.',\n",
       " 'and',\n",
       " 'mrs',\n",
       " '.',\n",
       " 'dursley',\n",
       " ',',\n",
       " 'of',\n",
       " 'number',\n",
       " 'four',\n",
       " ',',\n",
       " 'privet',\n",
       " 'drive',\n",
       " ',',\n",
       " 'were',\n",
       " 'proud',\n",
       " 'to',\n",
       " 'say',\n",
       " 'that']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1867"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['m',\n",
       " 'r',\n",
       " '.',\n",
       " 'and',\n",
       " 'mrs',\n",
       " '.',\n",
       " 'dursley',\n",
       " ',',\n",
       " 'of',\n",
       " 'number',\n",
       " 'four',\n",
       " ',',\n",
       " 'privet',\n",
       " 'drive',\n",
       " ',',\n",
       " 'were',\n",
       " 'proud',\n",
       " 'to',\n",
       " 'say',\n",
       " 'that',\n",
       " 'they',\n",
       " 'were',\n",
       " 'perfectly',\n",
       " 'normal',\n",
       " ',',\n",
       " 'thank',\n",
       " 'you',\n",
       " 'very',\n",
       " 'much',\n",
       " '.',\n",
       " 'they',\n",
       " 'were',\n",
       " 'the',\n",
       " 'last',\n",
       " 'people',\n",
       " 'you',\n",
       " 'd',\n",
       " 'expect',\n",
       " 'to',\n",
       " 'be',\n",
       " 'involved',\n",
       " 'in',\n",
       " 'anything',\n",
       " 'strange',\n",
       " 'or',\n",
       " 'mysterious',\n",
       " ',',\n",
       " 'because',\n",
       " 'they',\n",
       " 'just']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b4KjIrLz8Ryf",
    "outputId": "f4ace4f8-ec84-4556-cba5-87748562de3c"
   },
   "source": [
    "## 2.- Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NWW4RxWhpAZf"
   },
   "source": [
    "- Crea vocabulario y define tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "7-V2bNQqpLTE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs: [2206, 2, 93, 87, 13, 20]\n",
      "Decoded Text: hello , how are you ?\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "\n",
    "# Define a WordLevel tokenizer with unk_token\n",
    "tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "# Create a WordLevelTrainer and specify special tokens (including [UNK])\n",
    "trainer = WordLevelTrainer(special_tokens=[\"[UNK]\", \"[PAD]\"], min_frequency=1)\n",
    "\n",
    "# Train the tokenizer on your text data, using the trainer\n",
    "tokenizer.train_from_iterator(sentences, trainer=trainer)\n",
    "\n",
    "# Now you can encode your text\n",
    "text = \"hello, how are you?\"\n",
    "encoding = tokenizer.encode(text)\n",
    "\n",
    "# Access the token IDs\n",
    "print(\"Token IDs:\", encoding.ids)\n",
    "\n",
    "# Decode the token IDs back to words\n",
    "decoded_text = tokenizer.decode(encoding.ids)\n",
    "print(\"Decoded Text:\", decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 5745\n"
     ]
    }
   ],
   "source": [
    "vocab_size = tokenizer.get_vocab_size()\n",
    "print(f\"Vocabulary size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "0viwLWSUpGrk"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PAD_IDX = tokenizer.token_to_id(\"[PAD]\") \n",
    "PAD_IDX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 64]) torch.Size([64, 64])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_len):\n",
    "        self.texts = [' '.join(tokens) for tokens in texts]\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len + 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        encoding = self.tokenizer.encode(text)\n",
    "        input_ids = encoding.ids\n",
    "\n",
    "        # Padding\n",
    "        padding_length = self.max_len - len(input_ids)\n",
    "        if padding_length > 0:\n",
    "            input_ids = input_ids + ([tokenizer.token_to_id(\"[PAD]\")] * padding_length)\n",
    "        elif padding_length < 0:\n",
    "            input_ids = input_ids[:self.max_len]\n",
    "        x = torch.tensor(input_ids, dtype=torch.long)[:-1]\n",
    "        y = torch.tensor(input_ids, dtype=torch.long)[1:]\n",
    "        return x, y\n",
    "\n",
    "# Crea los datasets\n",
    "maxlen = 64\n",
    "batch_size= 64\n",
    "train_dataset = TextDataset(sentences, tokenizer, max_len=maxlen)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "train_batch, train_label = next(iter(train_loader))\n",
    "print(train_batch.shape, train_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   3,   17,  102,  293,    5,  293,    2,   19,  532,    3,   57,  631,\n",
       "          14,    8,  484,    3,  550,   62,   18,  182,   27,   48,   11,    8,\n",
       "         857, 3272,  188,   22,   41, 2952,    3,  191,   10,    8,  720,  914,\n",
       "          36,   27,    3,    4,  680, 1064,    3,   33,   54,   13,    2,  110,\n",
       "          20,   19,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_batch[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 188,    8,  144,  249,   14,  589,    2,    9,  246,  325,   46,    6,\n",
       "         987,    3,   17,  172,    6,   97,    6,    4, 1238,    2,    7,  142,\n",
       "         324,  727,   26,   48, 3222,   16,  409, 1726, 2062,    3,   17,   89,\n",
       "           2,   89,   18,   17,   20,   73,   86,  849,   13,   87,    6,   38,\n",
       "         568,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u_N6IRYsT4ry",
    "outputId": "a1d1522d-c8db-493b-fc77-e3549aeeb40a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.29 ms ± 110 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "train_batch, target_batch = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "aauQX2tFXD8K"
   },
   "outputs": [],
   "source": [
    "train_batch, target_batch = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ROw_9EjsT2Et",
    "outputId": "37555466-8faa-49bf-898e-e7374d2c66db"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 64]), torch.Size([64, 64]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_batch.shape, target_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PZ7f4DHJreIj"
   },
   "source": [
    "## 3.- Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 64, 5745]), torch.Size([64, 64]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim=128, model_dim=128):\n",
    "        super(RNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.rnn = nn.LSTM(input_size=emb_dim, \n",
    "                        hidden_size=model_dim, \n",
    "                        num_layers=1, \n",
    "                        batch_first=True)\n",
    "        self.fc1 = nn.Linear(model_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, (hidden, cell) = self.rnn(x)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "rnn = RNN(vocab_size)\n",
    "output_batch = rnn(train_batch)\n",
    "output_batch.shape, target_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MsJbLyZ6b_57"
   },
   "source": [
    "## 4.- Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "R5bmAldOcJn0"
   },
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    start = time.time()\n",
    "    running_loss = 0.0\n",
    "    model.train()\n",
    "    for inputs, labels in train_loader:\n",
    "        labels = labels.view(-1)\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        outputs = outputs.view(-1, outputs.size(-1))\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    return f'Time for epoch {epoch} is {time.time()-start:.4f} sec Train loss: {running_loss / len(train_loader):.4f}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, seed_text, device, maxlen):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        input_ids = tokenizer.encode(seed_text).ids\n",
    "        idx = torch.tensor(input_ids, dtype=torch.long)\n",
    "        idx = idx.reshape([1, -1])\n",
    "        maxlen = maxlen - idx.shape[-1]\n",
    "\n",
    "        for _ in range(maxlen):\n",
    "            idx = idx.to(device)\n",
    "            logits = model(idx)[:, -1, :]      \n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            _, idx_next = torch.topk(probs, k=1, dim=-1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "        txt = \" \".join(\n",
    "                    [tokenizer.id_to_token(idx[0, _]) for _ in range(maxlen)]\n",
    "                )\n",
    "    return txt\n",
    "\n",
    "start_token = 'voldemort'\n",
    "#generate(rnn, start_token, 'cuda', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EvsuJHMBlm9y",
    "outputId": "ff57e59e-6632-40b4-d81c-085cbcc20a25"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "wl9MQCp9TLCh"
   },
   "outputs": [],
   "source": [
    "rnn.to(device)\n",
    "optimizer = optim.Adam(rnn.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "gtR1RoSBkARg",
    "outputId": "5e15136d-9e74-4734-d9d0-307ddbcd59d0",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time for interval is 0.3656 sec\n",
      "Time for epoch 0 is 0.3655 sec Train loss: 7.8472\n",
      "Output: voldemort . he . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "\n",
      "Time for interval is 9.6977 sec\n",
      "Time for epoch 50 is 0.1865 sec Train loss: 4.0215\n",
      "Output: voldemort s got to the end of the door . harry had been a very good , but he was a bit of the stone , and the only one of the dursleys had a bit of the stone , said harry , and the only one of the dursleys had a bit of the stone , said harry , and the only\n",
      "\n",
      "Time for interval is 9.6902 sec\n",
      "Time for epoch 100 is 0.1875 sec Train loss: 3.2003\n",
      "Output: voldemort will be able to get the same , but he was going to be a bit of a lot . he was going to be a bit of a lot . he was going to be a bit of a lot . he was going to be a bit of a lot . and then , said ron . i m not\n",
      "\n",
      "Time for interval is 9.6628 sec\n",
      "Time for epoch 150 is 0.2018 sec Train loss: 2.5664\n",
      "Output: voldemort will be able to get the stone ? harry , ron , and harry was so angry ? he said , and harry was so angry ? he said , and harry was so angry ? he said , and harry was so angry ? he said , and harry was so angry ? he said , and harry was so angry\n",
      "\n",
      "Time for interval is 9.7977 sec\n",
      "Time for epoch 200 is 0.2040 sec Train loss: 2.0870\n",
      "Output: voldemort will be able to get the stone in the house , but harry had just finished telling him what he d never even if anyone was going to be . he s not going to be able to get out of the way . the dursleys had a bit of toast before the slytherins were already there , and he was going\n",
      "\n",
      "Time for interval is 9.5846 sec\n",
      "Time for epoch 250 is 0.1879 sec Train loss: 1.7188\n",
      "Output: voldemort will be able to pull free to get , that to be quiet around into the air , and a large pink face , the first time in his head was . the goblin was standing behind them . harry was so relieved to see his way to the owlery to the feast , and you have a little chat always flew\n",
      "\n",
      "Time for interval is 9.6747 sec\n",
      "Time for epoch 300 is 0.1888 sec Train loss: 1.4327\n",
      "Output: voldemort will be able to come and finish me off . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "\n",
      "Time for interval is 9.6731 sec\n",
      "Time for epoch 350 is 0.1885 sec Train loss: 1.2015\n",
      "Output: voldemort will be able to come and finish me off . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "\n",
      "Time for interval is 9.6005 sec\n",
      "Time for epoch 400 is 0.1925 sec Train loss: 1.0281\n",
      "Output: voldemort will be able to come and finish me off . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "\n",
      "Time for interval is 9.4854 sec\n",
      "Time for epoch 450 is 0.1879 sec Train loss: 0.8796\n",
      "Output: voldemort will be able to come and finish me off . . . . snape spat bitterly on the ground . harry had never seen an owl if they heard the other . it was as though they were looking at the stone , quirrell murmured , tapping his way down . the whole of term banquet was one of them as brightly\n"
     ]
    }
   ],
   "source": [
    "epochs = 500\n",
    "start = time.time()\n",
    "for epoch in range(epochs):\n",
    "    report = train(rnn, device, train_loader, optimizer, epoch)\n",
    "    if epoch % 50 == 0:\n",
    "        print(f'\\nTime for interval is {time.time()-start:.4f} sec')\n",
    "        start = time.time()\n",
    "        print(report)\n",
    "        generated_text = generate(rnn, start_token, device, maxlen)\n",
    "        print(f'Output: {generated_text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'voldemort will be able to come and finish me off . . . . snape spat bitterly on the ground . harry had never seen her look on if it was . and you mustn t go back . and this , said hermione , and they ran to the dursleys , harry had almost forgotten that the exam enchantments , and there s no big deal , because he was to keep him in the sunlight . quick . i m not going to be able to hold a quill for a week . he was starting to feel that the dursleys would never had . it was the dearest ambition of many to two ; was that quidditch match in a week . i m not going to be somewhere , there s a great muggle , like one thing they re in the same , they were sure at once , he said , and they weren t sure they d done so , said harry . i don t be too friendly he s going to sc no , they got the back of the chamber , and that s why yer late , is it ? said harry , who was looking for a last word to her . the speaker was a plump woman who was talking to be that simple . they hadn t found out how to get past fluffy . hermione , you know , i ll show you a good night he saw was on the windowsill , staring down at the lights of passing cars and wondering . they ate stale cornflakes and cold tinned tomatoes on harry s birthday . harry stared at the whisperers as though he could see a huge dudley tantrum coming on , began wolfing down his bacon as fast as possible in her arms . hovering level with the topmost branches of what looked like a large rock after what they were facing an archway cloak with his ear . all . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_token = 'voldemort'\n",
    "generated_text = generate(rnn, start_token, device, 1000)\n",
    "generated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio\n",
    "- Incrementar el tamaño del dataset utilizando todos los libros de _Harry Potter_.\n",
    "- Entrenar con diferentes métodos de de Tokenización."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
