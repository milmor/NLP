{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The MIT License (MIT) Copyright (c) 2022 Emilio Morales\n",
    "#\n",
    "# Permission is hereby granted, free of charge, to any person obtaining a copy of \n",
    "# this software and associated documentation files (the \"Software\"), to deal in the Software without \n",
    "# restriction, including without limitation the rights to use, copy, modify, merge, publish, \n",
    "# distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the \n",
    "# Software is furnished to do so, subject to the following conditions:\n",
    "#\n",
    "# The above copyright notice and this permission notice shall be included in all copies or \n",
    "# substantial portions of the Software.\n",
    "#\n",
    "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, \n",
    "# INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND \n",
    "# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES \n",
    "# OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN \n",
    "# CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "23dI5tEHRZ_P"
   },
   "source": [
    "# GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is a simple and clean implementation of GPT (Generative Pre-trained Transformer).  The complete code can be found in [this repository](https://github.com/milmor/GPT)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "O00JdIwIRZ_U",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' # Disable tensorflow debugging logs\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.- Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Download the file\n",
    "import pathlib\n",
    "\n",
    "path_to_zip = tf.keras.utils.get_file(\n",
    "    'spa-eng.zip', origin='http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip',\n",
    "    extract=True)\n",
    "\n",
    "path_to_file = pathlib.Path(path_to_zip).parent/'spa-eng/spa.txt'\n",
    "\n",
    "def load_data(path):\n",
    "    text = path.read_text(encoding='utf-8')\n",
    "\n",
    "    lines = text.splitlines()\n",
    "    pairs = [line.split('\\t') for line in lines]\n",
    "\n",
    "    inp = ['spanish sentence = ' + inp + ' english sentence = ' + targ + ' [END]' for targ, inp in pairs]\n",
    "\n",
    "    return inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spanish sentence = Si quieres sonar como un hablante nativo, debes estar dispuesto a practicar diciendo la misma frase una y otra vez de la misma manera en que un m√∫sico de banjo practica el mismo fraseo una y otra vez hasta que lo puedan tocar correctamente y en el tiempo esperado. english sentence = If you want to sound like a native speaker, you must be willing to practice saying the same sentence over and over in the same way that banjo players practice the same phrase over and over until they can play it correctly and at the desired tempo. [END]\n"
     ]
    }
   ],
   "source": [
    "ds = load_data(path_to_file)\n",
    "print(ds[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.- Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(ds)\n",
    "batch_size = 128\n",
    "\n",
    "text_ds = tf.data.Dataset.from_tensor_slices(ds).shuffle(BUFFER_SIZE)\n",
    "text_ds = text_ds.shuffle(BUFFER_SIZE).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "wYfPkJ9CRZ_Z",
    "tags": []
   },
   "outputs": [],
   "source": [
    "vocab_size = 30000  \n",
    "maxlen = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "q-rSlbIMRZ_Z",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "vectorize_layer = TextVectorization(\n",
    "    standardize=None,\n",
    "    max_tokens=vocab_size - 1,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=maxlen + 1,\n",
    ")\n",
    "vectorize_layer.adapt(text_ds)\n",
    "vocab = vectorize_layer.get_vocabulary() \n",
    "\n",
    "\n",
    "def preprocess(text):\n",
    "    text = tf.expand_dims(text, -1)\n",
    "    tokenized_sentences = vectorize_layer(text)\n",
    "    x = tokenized_sentences[:, :-1]\n",
    "    y = tokenized_sentences[:, 1:]\n",
    "    return x, y\n",
    "\n",
    "\n",
    "text_ds = text_ds.map(preprocess)\n",
    "text_ds = text_ds.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(128, 32), dtype=int64, numpy=\n",
       "array([[4, 2, 3, ..., 0, 0, 0],\n",
       "       [4, 2, 3, ..., 0, 0, 0],\n",
       "       [4, 2, 3, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [4, 2, 3, ..., 0, 0, 0],\n",
       "       [4, 2, 3, ..., 6, 0, 0],\n",
       "       [4, 2, 3, ..., 0, 0, 0]])>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_batch_x, test_batch_y = next(iter(text_ds))\n",
    "test_batch_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.- Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../img/dot_product.png\" width=\"500\"/>\n",
    "\n",
    "__Image source: Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). Attention is all you need. Advances in neural information processing systems, 30.__\n",
    "\n",
    "\\begin{equation}\n",
    "\\mbox{MultiHead}(Q, K, V) = \\text{Concat}(\\mbox{head}_1,\\mbox{head}_2,\\ldots,\\mbox{head}_h)W^O,\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\mbox{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V) = \\text{softmax}\\left[\\frac{QW_i^Q(KW_i^K)^T}{\\sqrt{d_k}}\\right]VW_i^V,\n",
    "\\label{eq:selfattention}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "skdQnFckRZ_W",
    "tags": []
   },
   "source": [
    "### Dot-product attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(layers.Layer):\n",
    "    def __init__(self, model_dim, n_heads, rate=0.1, initializer='glorot_uniform'):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.model_dim = model_dim\n",
    "\n",
    "        assert model_dim % self.n_heads == 0\n",
    "\n",
    "        self.head_dim = model_dim // self.n_heads\n",
    "\n",
    "        self.wq = layers.Dense(model_dim, kernel_initializer=initializer)\n",
    "        self.wk = layers.Dense(model_dim, kernel_initializer=initializer)\n",
    "        self.wv = layers.Dense(model_dim, kernel_initializer=initializer)\n",
    "        \n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "        \n",
    "        self.wo = layers.Dense(model_dim, kernel_initializer=initializer)\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.n_heads, self.head_dim))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, q, k, v, mask=None):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "\n",
    "        q = self.wq(q)  \n",
    "        k = self.wk(k)  \n",
    "        v = self.wv(v)  \n",
    "\n",
    "        q = self.split_heads(q, batch_size) \n",
    "        k = self.split_heads(k, batch_size)  \n",
    "        v = self.split_heads(v, batch_size) \n",
    "\n",
    "        dh = tf.cast(self.head_dim, tf.float32)\n",
    "        qk = tf.matmul(q, k, transpose_b=True)\n",
    "        scaled_qk =  qk / tf.math.sqrt(dh)\n",
    "        \n",
    "        if mask is not None:\n",
    "            scaled_qk += (mask * -1e9) \n",
    "\n",
    "        attn = self.dropout1(tf.nn.softmax(scaled_qk, axis=-1))\n",
    "        attn = tf.matmul(attn, v) \n",
    "\n",
    "        attn = tf.transpose(attn, perm=[0, 2, 1, 3]) \n",
    "        original_size_attention = tf.reshape(attn, (batch_size, -1, self.model_dim)) \n",
    "\n",
    "        output = self.dropout2(self.wo(original_size_attention))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Transformer block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "rjxrNdAPRZ_W",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 32, 128])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, emb_dim, n_heads=4, mlp_dim=512, \n",
    "                 rate=0.1, initializer='glorot_uniform', eps=1e-6, activation='gelu'):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attn = MultiHeadAttention(emb_dim, n_heads, initializer=initializer)\n",
    "        self.mlp = tf.keras.Sequential([\n",
    "            layers.Dense(mlp_dim, activation=activation, kernel_initializer=initializer), \n",
    "            layers.Dense(emb_dim, kernel_initializer=initializer),\n",
    "            layers.Dropout(rate)\n",
    "        ])\n",
    "        self.ln1 = layers.LayerNormalization(epsilon=eps)\n",
    "        self.ln2 = layers.LayerNormalization(epsilon=eps)\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        x = self.ln1(inputs)\n",
    "        x = inputs + self.attn(x, x, x, mask) \n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x\n",
    "    \n",
    "    \n",
    "emb_dim = 128\n",
    "test_layer = TransformerBlock(emb_dim)\n",
    "test_layer(tf.ones([1, maxlen, emb_dim])).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rWmuF3O9RZ_X"
   },
   "source": [
    "### Positional embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "iGo-OsMaRZ_X",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TokenEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, emb_dim, \n",
    "                 rate=0.1, initializer='glorot_uniform'):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.max_len = maxlen\n",
    "        self.token_emb = layers.Embedding(\n",
    "            input_dim=vocab_size, output_dim=emb_dim, \n",
    "            embeddings_initializer=initializer)\n",
    "        self.position_emb = layers.Embedding(\n",
    "            input_dim=maxlen, output_dim=emb_dim, \n",
    "            embeddings_initializer=initializer)\n",
    "        self.dropout = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x):\n",
    "        token_embeddings = self.token_emb(x)\n",
    "        positions = tf.range(start=0, limit=self.max_len, delta=1)\n",
    "        positions = self.position_emb(positions)\n",
    "        return self.dropout(token_embeddings + positions) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([128, 32, 30000])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class GPT(tf.keras.models.Model):\n",
    "    def __init__(self, vocab_size=20000, maxlen=512, \n",
    "                 emb_dim=256, heads=4, mlp_dim=128, depth=3, \n",
    "                 rate=0.2, initializer='glorot_uniform', \n",
    "                 embedding_initializer='glorot_uniform', eps=1e-6,\n",
    "                 mlp_activation='gelu'):\n",
    "        super(GPT, self).__init__()\n",
    "        self.depth = depth\n",
    "        self.tok_emb = TokenEmbedding(maxlen, vocab_size, \n",
    "                        emb_dim, rate=rate, initializer=embedding_initializer)\n",
    "        self.drop = layers.Dropout(rate)\n",
    "            \n",
    "        self.transformer = [TransformerBlock(emb_dim, \n",
    "                                heads, mlp_dim, rate=rate,\n",
    "                                initializer=initializer, eps=eps, \n",
    "                                activation=mlp_activation)\n",
    "                            for _ in range(depth)]\n",
    "\n",
    "        self.layernorm = layers.LayerNormalization(epsilon=eps)\n",
    "        self.out = layers.Dense(vocab_size, kernel_initializer=initializer)\n",
    "        \n",
    "    def get_padding_mask(self, seq):\n",
    "        seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "        # add extra dimensions to add the padding\n",
    "        # to the attention logits.\n",
    "        return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n",
    "\n",
    "    def get_attention_mask(self, size):\n",
    "        mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "        return mask  # (seq_len, seq_len)\n",
    "    \n",
    "    def create_mask(self, x):\n",
    "        attn_mask = self.get_attention_mask(tf.shape(x)[1])\n",
    "        padding_mask = self.get_padding_mask(x)\n",
    "        attn_mask = tf.maximum(padding_mask, attn_mask)\n",
    "        return attn_mask\n",
    "                       \n",
    "    def call(self, x):\n",
    "        mask = self.create_mask(x)\n",
    " \n",
    "        x = self.tok_emb(x)\n",
    "        x = self.drop(x)\n",
    "\n",
    "        for i in range(self.depth):\n",
    "            x = self.transformer[i](x, mask)\n",
    "\n",
    "        x = self.layernorm(x)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "       \n",
    "        \n",
    "emb_dim = 128\n",
    "depth = 3\n",
    "mlp_dim = 256\n",
    "\n",
    "gpt = GPT(maxlen=maxlen, vocab_size=vocab_size, emb_dim=emb_dim,\n",
    "            mlp_dim=mlp_dim, depth=depth)\n",
    "out = gpt(test_batch_x)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"gpt\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " token_embedding (TokenEmbed  multiple                 3844096   \n",
      " ding)                                                           \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         multiple                  0         \n",
      "                                                                 \n",
      " transformer_block_1 (Transf  multiple                 132480    \n",
      " ormerBlock)                                                     \n",
      "                                                                 \n",
      " transformer_block_2 (Transf  multiple                 132480    \n",
      " ormerBlock)                                                     \n",
      "                                                                 \n",
      " transformer_block_3 (Transf  multiple                 132480    \n",
      " ormerBlock)                                                     \n",
      "                                                                 \n",
      " layer_normalization_8 (Laye  multiple                 256       \n",
      " rNormalization)                                                 \n",
      "                                                                 \n",
      " dense_24 (Dense)            multiple                  3870000   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8,111,792\n",
      "Trainable params: 8,111,792\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gpt.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P-jkhvM3RZ_a",
    "tags": []
   },
   "source": [
    "## 4.- Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_to_ids = tf.keras.layers.StringLookup(\n",
    "                vocabulary=vectorize_layer.get_vocabulary(),\n",
    "                mask_token='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ids_to_text = tf.keras.layers.StringLookup(\n",
    "                vocabulary=vectorize_layer.get_vocabulary(),\n",
    "                mask_token='',\n",
    "                invert=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spanish sentence = Me gustan los perros rojos. english sentence = Hizo suponer suicidado. cama! rehusaron suicidado. president? Hizo suicidado. ayude. suicidado. ¬°Hay suicidado. suicidado. suicidado. rehusaron suicidado. compasi√≥n termina? pesadilla. suicidado.\n",
      "spanish sentence = Me encanta escribir. english sentence = quedito. Hizo suicidado. suponer suicidado. cama! rehusaron suicidado. president? Hizo suicidado. ayude. suicidado. ¬°Hay suicidado. suicidado. suicidado. rehusaron suicidado. compasi√≥n termina? pesadilla. suicidado.\n",
      "spanish sentence = Los elefantes comen manzanas. english sentence = Hizo Hizo suponer Hizo cama! Hizo coleccionaba suicidado. Hizo suicidado. ayude. suicidado. ¬°Hay suicidado. suicidado. suicidado. rehusaron rehusaron compasi√≥n termina? pesadilla. suicidado.\n"
     ]
    }
   ],
   "source": [
    "context = ['spanish sentence = Me gustan los perros rojos. english sentence = ',\n",
    "           'spanish sentence = Me encanta escribir. english sentence = ',\n",
    "           'spanish sentence = Los elefantes comen manzanas. english sentence = ']\n",
    "\n",
    "def sample(model, context, maxlen):  \n",
    "    words = [context.split()] # add batch dim\n",
    "    x = tf.cast(text_to_ids(words), tf.int32)\n",
    "    # Generate new text by sampling from the model\n",
    "    for i in range(x.shape[1], maxlen):\n",
    "        # Pad the input sequence to seq_len\n",
    "        x_pad = tf.keras.preprocessing.sequence.pad_sequences(x, maxlen=maxlen, padding=\"post\")\n",
    "        # Generate logits from the model\n",
    "        logits = model(x_pad, training=False)\n",
    "\n",
    "        pred_index = tf.argmax(logits[:, i-1, :], axis=-1, \n",
    "                               output_type=tf.dtypes.int32)\n",
    "        pred_index = pred_index[tf.newaxis]\n",
    "        if ids_to_text(pred_index) == '[END]':\n",
    "            break\n",
    "        # Concatenate the new token to the sequence\n",
    "        x = tf.concat([x, pred_index], axis=-1)\n",
    "\n",
    "    str_list = ids_to_text(x)[0].numpy()\n",
    "    text = ' '.join([s.decode('utf-8') for s in str_list])\n",
    "    return text\n",
    "\n",
    "for c in context:\n",
    "    trans = sample(gpt, c, maxlen)\n",
    "    print(f\"{trans}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(0.001, beta_1=0.9, beta_2=0.999)\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def loss_function(label, pred):\n",
    "    mask = label != 0\n",
    "    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "    loss = loss_object(label, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss.dtype)\n",
    "    loss *= mask\n",
    "\n",
    "    loss = tf.reduce_sum(loss)/tf.reduce_sum(mask)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, tar):\n",
    "    with tf.GradientTape() as tape:\n",
    "        pred = gpt(inp, training=True)\n",
    "        loss = loss_function(tar, pred)\n",
    "    gradients = tape.gradient(loss, gpt.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, gpt.trainable_variables))\n",
    "    train_loss(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for epoch 1 is: 36.89 secs Loss: 4.1995\n",
      "Time taken for epoch 2 is: 18.92 secs Loss: 3.0519\n",
      "Output: \n",
      "spanish sentence = Me gustan los perros rojos. english sentence = I like dogs.\n",
      "spanish sentence = Me encanta escribir. english sentence = I love to sleep.\n",
      "spanish sentence = Los elefantes comen manzanas. english sentence = The children are [UNK]\n",
      "Time taken for epoch 3 is: 18.64 secs Loss: 2.6254\n",
      "Time taken for epoch 4 is: 18.29 secs Loss: 2.3875\n",
      "Output: \n",
      "spanish sentence = Me gustan los perros rojos. english sentence = I like dogs.\n",
      "spanish sentence = Me encanta escribir. english sentence = I love writing.\n",
      "spanish sentence = Los elefantes comen manzanas. english sentence = The birds eat apples.\n",
      "Time taken for epoch 5 is: 17.84 secs Loss: 2.2406\n",
      "Time taken for epoch 6 is: 18.62 secs Loss: 2.1420\n",
      "Output: \n",
      "spanish sentence = Me gustan los perros rojos. english sentence = I like red dogs.\n",
      "spanish sentence = Me encanta escribir. english sentence = I love writing.\n",
      "spanish sentence = Los elefantes comen manzanas. english sentence = Elephants eat apples.\n",
      "Time taken for epoch 7 is: 18.59 secs Loss: 2.0705\n",
      "Time taken for epoch 8 is: 18.16 secs Loss: 2.0160\n",
      "Output: \n",
      "spanish sentence = Me gustan los perros rojos. english sentence = I like red dogs.\n",
      "spanish sentence = Me encanta escribir. english sentence = I love to write.\n",
      "spanish sentence = Los elefantes comen manzanas. english sentence = Elephants eat apples.\n",
      "Time taken for epoch 9 is: 17.79 secs Loss: 1.9707\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "\n",
    "for epoch in range(1, epochs):\n",
    "    start = time.time()\n",
    "    train_loss.reset_states()\n",
    "    for (batch, (inp, tar)) in enumerate(text_ds):\n",
    "        train_step(inp, tar)\n",
    "    \n",
    "    print(f'Time taken for epoch {epoch} is: {time.time() - start:.2f} secs', end=' ')\n",
    "    print(f'Loss: {train_loss.result():.4f}')\n",
    "    \n",
    "    if epoch % 2 == 0:\n",
    "        print('Output: ')\n",
    "        for c in context:\n",
    "            trans = sample(gpt, c, maxlen)\n",
    "            print(f\"{trans}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
