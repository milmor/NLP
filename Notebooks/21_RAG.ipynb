{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0d28f01-7c24-4442-87b2-b435dc6d1698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The MIT License (MIT) Copyright (c) 2025 Emilio Morales\n",
    "#\n",
    "# Permission is hereby granted, free of charge, to any person obtaining a copy of \n",
    "# this software and associated documentation files (the \"Software\"), to deal in the Software without \n",
    "# restriction, including without limitation the rights to use, copy, modify, merge, publish, \n",
    "# distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the \n",
    "# Software is furnished to do so, subject to the following conditions:\n",
    "#\n",
    "# The above copyright notice and this permission notice shall be included in all copies or \n",
    "# substantial portions of the Software.\n",
    "#\n",
    "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, \n",
    "# INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND \n",
    "# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES \n",
    "# OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN \n",
    "# CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc32deb-6297-4ffa-8472-2eda23ef363c",
   "metadata": {
    "id": "bc1957fd-54a8-4433-9716-71e29f8b84ef"
   },
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/milmor/NLP/blob/main/Notebooks/21_RAG.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />\n",
    "    Run in Google Colab</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c141baa5-e27d-487b-b20d-ac9891c7310f",
   "metadata": {
    "id": "c141baa5-e27d-487b-b20d-ac9891c7310f"
   },
   "source": [
    "# RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8bc43f-ecb7-40a1-b9a3-5e5367611a20",
   "metadata": {},
   "source": [
    "- Harry Potter book: https://www.kaggle.com/datasets/shubhammaindola/harry-potter-books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0R1a5TROqug0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "0R1a5TROqug0",
    "outputId": "9836f34a-2e01-4b19-cf52-1eeb2426b01b",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install -q sentence-transformers faiss-cpu\n",
    "#!pip install vllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84a30480-ab19-4bf7-87c5-cb8be025d1e1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "84a30480-ab19-4bf7-87c5-cb8be025d1e1",
    "outputId": "8e62123b-389d-44dd-d194-a2bb1a8a90c4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.5.1+cu124'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "XtWKoKiwScwB",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XtWKoKiwScwB",
    "outputId": "e41a50b2-0fa2-43a7-960a-4dbc2bcbb55f"
   },
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2qjndwcPFz5R",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2qjndwcPFz5R",
    "outputId": "ab67a81a-b91b-4c1d-e71d-046f3163c124"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks created: 157\n",
      "First chunk: M r. and Mrs. Dursley, of number four, Privet Drive, were proud to say that they were perfectly normal, thank you very much. They were the last people you’d expect to be involved in anything strange o...\n"
     ]
    }
   ],
   "source": [
    "def chunk_text(text, chunk_size):\n",
    "    \"\"\"Splits text into chunks of a specified size.\"\"\"\n",
    "    words = text.split()\n",
    "    chunks = [' '.join(words[i:i + chunk_size]) for i in range(0, len(words), chunk_size)]\n",
    "    return chunks\n",
    "\n",
    "with open('./01 Harry Potter and the Sorcerers Stone.txt', 'r', encoding='utf-8') as f:\n",
    "    book_text = f.read()\n",
    "\n",
    "chunk_size = 500  # You can adjust the chunk size as needed\n",
    "book_chunks = chunk_text(book_text, chunk_size)\n",
    "\n",
    "print(f\"Number of chunks created: {len(book_chunks)}\")\n",
    "print(f\"First chunk: {book_chunks[0][:200]}...\") # Print the first 200 characters of the first chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25160ee9-f3c0-4e81-95ad-14120dba9474",
   "metadata": {},
   "source": [
    "## Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "-Umynz-kHqac",
   "metadata": {
    "id": "-Umynz-kHqac"
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers.util import cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "zGr4MlmyGJ2b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 517
    },
    "id": "zGr4MlmyGJ2b",
    "outputId": "3a650084-3501-4f52-b5e7-dbf1134711c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8852]])\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer(\n",
    "    \"all-MiniLM-L6-v2\" # Using a smaller model\n",
    ")\n",
    "\n",
    "# The actual max sequence length for this model is 512\n",
    "model.max_seq_length = 512\n",
    "\n",
    "embeddings = model.encode([\n",
    "    'How is the weather today?',\n",
    "    'What is the current weather like today?'\n",
    "])\n",
    "print(cos_sim(embeddings[0], embeddings[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "XO2SoeOsKc_e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XO2SoeOsKc_e",
    "outputId": "61e4a40e-b2e1-4403-9978-e8ed9b772b53"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(384,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "zKAtRsrqHLP4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zKAtRsrqHLP4",
    "outputId": "23c5ce91-c624-4956-b18f-36b1f1b2e438"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8852]])\n"
     ]
    }
   ],
   "source": [
    "embeddings = model.encode([\n",
    "    'How is the weather today?',\n",
    "    'What is the current weather like today?'\n",
    "])\n",
    "print(cos_sim(embeddings[0], embeddings[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "RZnlcrH5G25q",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RZnlcrH5G25q",
    "outputId": "11367e42-2059-4fad-e4ad-6fd96de5c314"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0808]])\n"
     ]
    }
   ],
   "source": [
    "embeddings = model.encode([\n",
    "    'How is the weather today?',\n",
    "    \"What's the name of your dog\"\n",
    "])\n",
    "\n",
    "print(cos_sim(embeddings[0], embeddings[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "zK-SnXzjH2l7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zK-SnXzjH2l7",
    "outputId": "47ac56dd-aa0a-4e27-bdea-a1cc849f8228"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'text'],\n",
       "    num_rows: 157\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_dict = {\n",
    "    \"id\": list(range(len(book_chunks))),\n",
    "    \"text\": book_chunks}\n",
    "\n",
    "book = Dataset.from_dict(my_dict)\n",
    "book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "lYbBMeKEHYUx",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49
    },
    "id": "lYbBMeKEHYUx",
    "outputId": "e5293aea-3452-4df3-8244-e67de89dc355"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaa83bea50134315a046f4471fe47e88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/157 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def embed(batch):\n",
    "  # Explicitly truncate text to the model's max sequence length\n",
    "  truncated_texts = [text[:model.max_seq_length] for text in batch[\"text\"]]\n",
    "  info = model.encode(truncated_texts)\n",
    "  return {\"embeddings\": info}\n",
    "\n",
    "dataset = book.map(embed, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "oa_T0YayMIu_",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oa_T0YayMIu_",
    "outputId": "ff0d32aa-dd81-466c-a6eb-dc67d6f9a5ec"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'text', 'embeddings'],\n",
       "    num_rows: 157\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "oZyIEBBvIFn3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49
    },
    "id": "oZyIEBBvIFn3",
    "outputId": "ffb8cf75-3af8-4240-e792-55a5a9548072"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "664c9a324db94a228176370edd4cdcfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = dataset.add_faiss_index(column=\"embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0Y9l8WPf_dke",
   "metadata": {
    "id": "0Y9l8WPf_dke"
   },
   "outputs": [],
   "source": [
    "def search(query, k):\n",
    "  emb_query = model.encode([query])\n",
    "  return data.get_nearest_examples(\"embeddings\", emb_query, k=k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "K3quE4WWUFG9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K3quE4WWUFG9",
    "outputId": "5c721f87-8b3d-4983-9f09-9588d6055f92"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['with a mixture of shock and suspicion. “Professor Dumbledore will be back tomorrow,” she said finally. I don’t know how you found out about the Stone, but rest assured, no one can possibly steal it, it’s too well protected.” “But Professor —” “Potter, I know what I’m talking about,” she said shortly. She bent down and gathered up the fallen books. I suggest you all go back outside and enjoy the sunshine.” But they didn’t. “It’s tonight,” said Harry, once he was sure Professor McGonagall was out of earshot. “Snape’s going through the trapdoor tonight. He’s found out everything he needs, and now he’s got Dumbledore out of the way. He sent that note, I bet the Ministry of Magic will get a real shock when Dumbledore turns up.” “But what can we —” Hermione gasped. Harry and Ron wheeled round. Snape was standing there. “Good afternoon,” he said smoothly. They stared at him. “You shouldn’t be inside on a day like this,” he said, with an odd, twisted smile. “We were —” Harry began, without any idea what he was going to say. “You want to be more careful,” said Snape. “Hanging around like this, people will think you’re up to something. And Gryffindor really can’t afford to lose any more points, can it?” Harry flushed. They turned to go outside, but Snape called them back. “Be warned, Potter — any more nighttime wanderings and I will personally make sure you are expelled. Good day to you.” He strode off in the direction of the staffroom. Out on the stone steps, Harry turned to the others. “Right, here’s what we’ve got to do,” he whispered urgently. “One of us has got to keep an eye on Snape — wait outside the staff room and follow him if he leaves it. Hermione, you’d better do that.” “Why me?” “It’s obvious,” said Ron. “You can pretend to be waiting for Professor Flitwick, you know.” He put on a high voice, “‘Oh Professor Flitwick, I’m so worried, I think I got question fourteen b wrong….’” “Oh, shut up,” said Hermione, but she agreed to go and watch out for Snape. “And we’d better stay outside the third-floor corridor,” Harry told Ron. “Come on.” But that part of the plan didn’t work. No sooner had they reached the door separating Fluffy from the rest of the school than Professor McGonagall turned up again and this time, she lost her temper. “I suppose you think you’re harder to get past than a pack of enchantments!” she stormed. “Enough of this nonsense! If I hear you’ve come anywhere near here again, I’ll take another fifty points from Gryffindor! Yes, Weasley, from my own house!” Harry and Ron went back to the common room, Harry had just said, “At least Hermione’s on Snape’s tail,” when the portrait of the Fat Lady swung open and Hermione came in. “I’m sorry, Harry!” she wailed. “Snape came out and asked me what I was doing, so I said I',\n",
       " 'was waiting for Flitwick, and Snape went to get him, and I’ve only just got away, I don’t know where Snape went.” “Well, that’s it then, isn’t it?” Harry said. The other two stared at him. He was pale and his eyes were glittering. “I’m going out of here tonight and I’m going to try and get to the Stone first.” “You’re mad!” said Ron. “You can’t!” said Hermione. “After what McGonagall and Snape have said? You’ll be expelled!” “SO WHAT” Harry shouted. “Don’t you understand? If Snape gets hold of the Stone, Voldemort’s coming back! Haven’t you heard what it was like when he was trying to take over? There won’t be any Hogwarts to get expelled from! He’ll flatten it, or turn it into a school for the Dark Arts! Losing points doesn’t matter anymore, can’t you see? D’you think he’ll leave you and your families alone if Gryffindor wins the house cup? If I get caught before I can get to the Stone, well, I’ll have to go back to the Dursleys and wait for Voldemort to find me there, it’s only dying a bit later than I would have, because I’m never going over to the Dark Side! I’m going through that trapdoor tonight and nothing you two say is going to stop me! Voldemort killed my parents, remember?” He glared at them. “You’re right Harry,” said Hermione in a small voice. “I’ll use the invisibility cloak,” said Harry. “It’s just lucky I got it back.” “But will it cover all three of us?” said Ron. “All — all three of us?” “Oh, come off it, you don’t think we’d let you go alone?” “Of course not,” said Hermione briskly. “How do you think you’d get to the Stone without us? I’d better go and took through my books, there might be something useful….” “But if we get caught, you two will be expelled, too.” “Not if I can help it,” said Hermione grimly. “Flitwick told me in secret that I got a hundred and twelve percent on his exam. They’re not throwing me out after that.” After dinner the three of them sat nervously apart in the common room. Nobody bothered them; none of the Gryffindors had anything to say to Harry any more, after all. This was the first night he hadn’t been upset by it. Hermione was skimming through all her notes, hoping to come across one of the enchantments they were about to try to break. Harry and Ron didn’t talk much. Both of them were thinking about what they were about to do. Slowly, the room emptied as people drifted off to bed. “Better get the cloak,” Ron muttered, as Lee Jordan finally left, stretching and yawning. Harry ran upstairs to their dark dormitory. He pulled out the cloak and then his eyes fell on the flute Hagrid had given him for Christmas. He pocketed it to use on Fluffy — he didn’t feel much like singing. He ran back down',\n",
       " 'opened them again. He saw his reflection, pale and scared-looking at first. But a moment later, the reflection smiled at him. It put its hand into its pocket and pulled out a blood-red stone. It winked and put the Stone back in its pocket — and as it did so, Harry felt something heavy drop into his real pocket. Somehow — incredibly — he’d gotten the Stone. “Well?” said Quirrell impatiently. “What do you see?” Harry screwed up his courage. “I see myself shaking hands with Dumbledore,” he invented. “I — I’ve won the house cup for Gryffindor.” Quirrell cursed again. “Get out of the way,” he said. As Harry moved aside, he felt the Sorcerer’s Stone against his leg. Dare he make a break for it? But he hadn’t walked five paces before a high voice spoke, though Quirrell wasn’t moving his lips. “He lies…He lies.…” “Potter, come back here!” Quirrell shouted. “Tell me the truth! What did you just see?” The high voice spoke again. “Let me speak to him…face-to-face…” “Master, you are not strong enough!” “I have strength enough…for this…” Harry felt as if Devil’s Snare was rooting him to the spot. He couldn’t move a muscle. Petrified, he watched as Quirrell reached up and began to unwrap his turban. What was going on? The turban fell away. Quirrell’s head looked strangely small without it. Then he turned slowly on the spot. Harry would have screamed, but he couldn’t make a sound. Where there should have been a back to Quirrell’s head, there was a face, the most terrible face Harry had ever seen. It was chalk white with glaring red eyes and slits for nostrils, like a snake. “Harry Potter…” it whispered. Harry tried to take a step backward but his legs wouldn’t move. “See what I have become?” the face said. “Mere shadow and vapor….I have form only when I can share another’s body…but there have always been those willing to let me into their hearts and minds…Unicorn blood has strengthened me, these past weeks…you saw faithful Quirrell drinking it for me in the forest…and once I have the Elixir of Life, I will be able to create a body of my own….Now…why don’t you give me that Stone in your pocket?” So he knew. The feeling suddenly surged back into Harry’s legs. He stumbled backward. “Don’t be a fool,” snarled the face. “Better save your own life and join me…or you’ll meet the same end as your parents…They died begging me for mercy…” “LIAR!” Harry shouted suddenly. Quirrell was walking backward at him, so that Voldemort could still see him. The evil face was now smiling. “How touching…” it hissed. “I always value bravery….Yes, boy, your parents were brave…I killed your father first; and he put up a courageous fight…but your mother needn’t have died…she was trying to protect you…Now give me the Stone, unless you want her to have died in vain.” “NEVER!” Harry sprang toward the flame door, but Voldemort screamed “SEIZE']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores, result = search(\"Why does Voldemort want the stone?\", 3)\n",
    "result['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6o_gf4xCU_q2",
   "metadata": {
    "id": "6o_gf4xCU_q2"
   },
   "source": [
    "## LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "zSXnAdqmU-8y",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 236
    },
    "id": "zSXnAdqmU-8y",
    "outputId": "345a20ad-7e40-4fc9-a2ee-b715da7514d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-14 14:17:14 config.py:510] This model supports multiple tasks: {'embed', 'score', 'reward', 'classify', 'generate'}. Defaulting to 'generate'.\n",
      "INFO 11-14 14:17:14 gptq_marlin.py:109] The model is convertible to gptq_marlin during runtime. Using gptq_marlin kernel.\n",
      "INFO 11-14 14:17:14 llm_engine.py:234] Initializing an LLM engine (v0.6.6.post1) with config: model='neuralmagic/Meta-Llama-3.1-8B-Instruct-quantized.w4a16', speculative_config=None, tokenizer='neuralmagic/Meta-Llama-3.1-8B-Instruct-quantized.w4a16', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=gptq_marlin, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=neuralmagic/Meta-Llama-3.1-8B-Instruct-quantized.w4a16, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"candidate_compile_sizes\":[],\"compile_sizes\":[],\"capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "INFO 11-14 14:17:15 selector.py:120] Using Flash Attention backend.\n",
      "INFO 11-14 14:17:15 model_runner.py:1094] Starting to load model neuralmagic/Meta-Llama-3.1-8B-Instruct-quantized.w4a16...\n",
      "INFO 11-14 14:17:15 gptq_marlin.py:200] Using MarlinLinearKernel for GPTQMarlinLinearMethod\n",
      "INFO 11-14 14:17:16 weight_utils.py:251] Using model weights format ['*.safetensors']\n",
      "INFO 11-14 14:17:16 weight_utils.py:296] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c024fd04800349b3b34d62dfbf4876ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-14 14:17:17 model_runner.py:1099] Loading model weights took 5.3555 GB\n",
      "INFO 11-14 14:17:19 worker.py:241] Memory profiling takes 2.05 seconds\n",
      "INFO 11-14 14:17:19 worker.py:241] the current vLLM instance can use total_gpu_memory (23.68GiB) x gpu_memory_utilization (0.90) = 21.31GiB\n",
      "INFO 11-14 14:17:19 worker.py:241] model weights take 5.36GiB; non_torch_memory takes 0.01GiB; PyTorch activation peak memory takes 1.22GiB; the rest of the memory reserved for KV Cache is 14.72GiB.\n",
      "INFO 11-14 14:17:19 gpu_executor.py:76] # GPU blocks: 7538, # CPU blocks: 2048\n",
      "INFO 11-14 14:17:19 gpu_executor.py:80] Maximum concurrency for 8192 tokens per request: 14.72x\n",
      "INFO 11-14 14:17:21 model_runner.py:1415] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████████| 35/35 [00:12<00:00,  2.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-14 14:17:33 model_runner.py:1535] Graph capturing finished in 12 secs, took 0.26 GiB\n",
      "INFO 11-14 14:17:33 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 15.47 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "#model_id = \"neuralmagic/Meta-Llama-3.1-8B-Instruct-FP8-dynamic\"\n",
    "model_id = \"neuralmagic/Meta-Llama-3.1-8B-Instruct-quantized.w4a16\"\n",
    "\n",
    "number_gpus = 1\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "max_model_len = 8192\n",
    "\n",
    "sampling_params = SamplingParams(temperature=0.6, top_p=0.9, max_tokens=256)\n",
    "llm = LLM(model=model_id, tensor_parallel_size=number_gpus, max_model_len=max_model_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "QeC-6NXMZ_HX",
   "metadata": {
    "id": "QeC-6NXMZ_HX"
   },
   "outputs": [],
   "source": [
    "def format_prompt_chatbot(prompt, docs, k):\n",
    "  context = \"\\n\".join(docs['text'][:k])\n",
    "  full_prompt = f\"Question: {prompt} \\nContext: {context}\"\n",
    "  return {\"role\": \"user\", \"content\": full_prompt}\n",
    "\n",
    "def generate(message):\n",
    "  prompts = tokenizer.apply_chat_template(message, tokenize=False, add_generation_prompt=True)\n",
    "  outputs = llm.generate(prompts, sampling_params)\n",
    "  return outputs[0].outputs[0].text\n",
    "\n",
    "def rag_llama(prompt, k=3):\n",
    "  scores, docs = search(prompt, k)\n",
    "\n",
    "  user_message = format_prompt_chatbot(prompt, docs, k)\n",
    "  system_message = {\"role\": \"system\", \"content\": \"You are a helpful assistant for answering questions. \"\n",
    "  \" You are given the extracted parts of a long document. Provide a conversational answer. \\n\"}\n",
    "  message = [system_message, user_message]\n",
    "\n",
    "  return generate(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "jdy8fGBtax39",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "id": "jdy8fGBtax39",
    "outputId": "9ae93f4e-59ce-4633-a2bb-f2506bf0d575"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█| 1/1 [00:01<00:00,  1.04s/it, est. speed input: 2024.0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oh my, Voldemort's plan is to get the Sorcerer's Stone. He's using Quirrell as a puppet to get to it. The Stone is a powerful object that can grant eternal life, and Voldemort is desperate to get it to become immortal and gain the power to return to power.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "output = rag_llama(\"Why does Voldemort want the stone?\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "Hks2_qS2dpEi",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "id": "Hks2_qS2dpEi",
    "outputId": "994ca611-d8c2-43c1-e2be-9c8130072ae6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█| 1/1 [00:02<00:00,  2.75s/it, est. speed input: 767.29"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It seems like you want me to explain Voldemort's motivations for wanting the Sorcerer's Stone. Well, the main reason Voldemort wants the Stone is to gain the power to create a physical body for himself. \n",
      "\n",
      "As we see in the conversation, the face that is revealed to be Voldemort's is a spirit or a form that can only exist by possessing the bodies of others. He is using Quirrell's body to carry out his plans, but he needs the Stone to create a permanent, physical body for himself. \n",
      "\n",
      "The Stone is also associated with the Elixir of Life, which is a powerful potion that can grant eternal life. Voldemort wants to use this Elixir to become immortal and invulnerable. He believes that with the Stone and the Elixir, he will be able to achieve his goal of immortality and become the most powerful wizard of all time. \n",
      "\n",
      "Additionally, Voldemort's ultimate goal is to return to power and dominate the wizarding world. He wants to create a Dark Empire, and the Sorcerer's Stone is a crucial step in achieving this goal. He is willing to go to any lengths to get it, including manipulating and deceiving others, and even killing those who stand in his way.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "output = rag_llama(\"Why does Voldemort want the stone?\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "CFUrvtbWeJUj",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "id": "CFUrvtbWeJUj",
    "outputId": "9b8d6bab-3256-4e3d-c5db-84c21e3c48e3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█| 1/1 [00:01<00:00,  1.35s/it, est. speed input: 1550.1"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Harry and Ron became friends on the train because they were introduced by Ron's twin brothers, Fred and George Weasley. The twins were sitting in a compartment, and they introduced themselves to Harry, and then they left, leaving Harry and Ron alone together. Ron was the first to speak, asking Harry if he was really Harry Potter, and Harry confirmed it. Ron was surprised to learn that Harry was the boy who had survived the attempt on his life by Lord Voldemort.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "output = rag_llama(\"How did Harry and Ron become friends on the train?\")\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
