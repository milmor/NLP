{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "316fd0ae-ef28-4452-8b3a-ba304e0e811b",
   "metadata": {},
   "source": [
    "- Game of thrones book: https://www.kaggle.com/datasets/khulasasndh/game-of-thrones-books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fccba23-1590-4a3a-95ee-bf84561cf77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' # Disable tensorflow debugging logs\n",
    "import tensorflow as tf\n",
    "import tensorflow_text as tf_text\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aff8a1c-af29-48ae-8ed0-c40b5b1b31f9",
   "metadata": {},
   "source": [
    "- Convertir documento a minúsculas para reducir el tamaño del vocabulario y obtener número de palabras "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c897b75-d93a-4ccb-845e-09aa0c0ce000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words: 1628063\n"
     ]
    }
   ],
   "source": [
    "path = './001ssb.txt'\n",
    "book = open(path, 'rb').read().decode(encoding='utf-8').lower()\n",
    "\n",
    "print(f'Words: {len(book)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8de59a7-c746-49dd-8237-83c9456df51e",
   "metadata": {},
   "source": [
    "## Pipeline\n",
    "- Preprocesamiento del texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38314433-fdc0-4ed5-9744-ff9382ce9b01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[b'A', b'game', b'of', b'thrones', b',', b'jon', b'and', b'sansa', b'.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = tf_text.UnicodeScriptTokenizer()\n",
    "tokens = tokenizer.tokenize([\"A game of thrones, jon and sansa.\"]).to_list()\n",
    "tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a358f2fb-6dc5-44ac-8dec-d1ac3c1591be",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[b'a',\n",
       " b'game',\n",
       " b'of',\n",
       " b'thrones',\n",
       " b'book',\n",
       " b'one',\n",
       " b'of',\n",
       " b'a',\n",
       " b'song',\n",
       " b'of']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book_words =  tokenizer.tokenize([book]).to_list()[0]\n",
    "book_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48036c9e-983a-4f25-a15c-94c110f3327b",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_ds = tf.data.Dataset.from_tensor_slices(book_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ddc21451-450e-443c-9e49-c0e832f3734a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'a'\n",
      "b'game'\n",
      "b'of'\n",
      "b'thrones'\n",
      "b'book'\n",
      "b'one'\n",
      "b'of'\n",
      "b'a'\n",
      "b'song'\n",
      "b'of'\n",
      "b'ice'\n",
      "b'and'\n",
      "b'fire'\n",
      "b'by'\n",
      "b'george'\n",
      "b'r'\n",
      "b'.'\n",
      "b'r'\n",
      "b'.'\n",
      "b'martin'\n"
     ]
    }
   ],
   "source": [
    "for words in words_ds.take(20):\n",
    "    print(words.numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f76da24-1ddd-4e95-a897-f82b51784146",
   "metadata": {},
   "source": [
    "- Generar lotes de oraciones y definir longitud de secuencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4dbe1fbf-d05a-41e5-aac1-fad0663172d6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'a' b'game' b'of' b'thrones' b'book' b'one' b'of' b'a' b'song' b'of'\n",
      " b'ice' b'and' b'fire' b'by' b'george' b'r' b'.' b'r' b'.' b'martin'\n",
      " b'prologue' b'\"' b'we' b'should' b'start' b'back' b',\"' b'gared' b'urged'\n",
      " b'as' b'the' b'woods' b'began' b'to' b'grow' b'dark' b'around' b'them'\n",
      " b'.\"' b'the' b'wildlings' b'are' b'dead' b'.\"\"' b'do' b'the' b'dead'\n",
      " b'frighten' b'you' b'?\"' b'ser']\n"
     ]
    }
   ],
   "source": [
    "seq_length = 50\n",
    "words_batches = words_ds.batch(seq_length+1, \n",
    "                               drop_remainder=True)\n",
    "\n",
    "for words in words_batches.take(1):\n",
    "    print(words.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be5bac8-9231-4875-862a-baa284494425",
   "metadata": {},
   "source": [
    "- Utiliza __join__ para que cada tensor del batch sea una sola cadena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38ecd4b6-410a-4151-abb5-59704830a2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_strings(tokens):\n",
    "    text = tf.strings.reduce_join(tokens, axis=0, separator=' ')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d72d5714-b410-4a31-a634-7239f4b35b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train_ds = words_batches.map(join_strings)\n",
    "batch_size = 32\n",
    "BUFFER_SIZE = len(raw_train_ds)\n",
    "\n",
    "raw_train_ds = (\n",
    "    raw_train_ds\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(batch_size, drop_remainder=True)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b91913fa-cfda-46d1-850e-ee4b03605dfb",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b'this is no toy ,\" he told her .\" be careful you don \\' t cut yourself . the edges are sharp enough to shave with .\"\" girls don \\' t shave ,\" arya said .\" maybe they should . have you ever seen the septa \\' s legs ?\" she'\n",
      " b'the jousting began , the day belonged to rhaegar targaryen . the crown prince wore the armor he would die in : gleaming black plate with the three - headed dragon of his house wrought in rubies on the breast . a plume of scarlet silk streamed behind him when he'\n",
      " b', where the metal had been folded back on itself a hundred times in the forging . catelyn had no love for swords , but she could not deny that ice had its own beauty . it had been forged in valyria , before the doom had come to the old'\n",
      " b'he waited behind the lord steward . the air was colder than a tomb , and more still . he felt a strange relief when they reemerged into the afternoon light on the north side of the wall . sam blinked at the sudden glare and looked around apprehensively .\" the'\n",
      " b'he was polite enough .\" my lord father has sent me to greet you , and inquire as to who leads this mighty host .\"\" i do .\" robb spurred his horse forward . he was in his armor , with the direwolf shield of winterfell strapped to his saddle and'\n",
      " b'brother ?\" dany asked .\" he ought to have come by now , for the feast .\"\" i saw his grace this morning ,\" he told her .\" he told me he was going to the western market , in search of wine .\"\" wine ?\" dany said doubtfully . viserys'\n",
      " b'explained .\" mychel redfort . he \\' s squire to ser lyn corbray . we \\' re to wed as soon as he becomes a knight , next year or the year after .\" she sounded so like sansa , so happy and innocent with her dreams . catelyn smiled ,'\n",
      " b'chamber was richly furnished . myrish carpets covered the floor instead of rushes , and in one corner a hundred fabulous beasts cavorted in bright paints on a carved screen from the summer isles . the walls were hung with tapestries from norvos and qohor and lys , and a pair'\n",
      " b'blind .\" an albino ,\" theon greyjoy said with wry amusement .\" this one will die even faster than the others .\" jon snow gave his father \\' s ward a long , chilling look .\" i think not , greyjoy ,\" he said .\" this one belongs to me .\"'\n",
      " b'you get us all killed .\" viserys laughed .\" they can \\' t kill us . they can \\' t shed blood here in the sacred city ... but i can .\" he laid the point of his sword between daenerys \\' s breasts and slid it downward , over the'\n",
      " b'rode , and it seemed no lance could touch him . brandon fell to him , and bronze yohn royce , and even the splendid ser arthur dayne , the sword of the morning . robert had been jesting with jon and old lord hunter as the prince circled the field'\n",
      " b'.\" did that surprise you , father ?\" he asked .\" did it upset your plans ? we were supposed to be butchered , were we not ?\" lord tywin drained his cup , his face expressionless .\" i put the least disciplined men on the left , yes . i'\n",
      " b'sun ,\" dany whispered .\" the sun warmed them as they rode .\" she commanded her handmaids to prepare her a bath . doreah built a fire outside the tent , while irri and jhiqui fetched the big copper tub - another bride gift - from the packhorses and carried water'\n",
      " b'ser raymun darry took up the tale .\" at wendish town , the people sought shelter in their holdfast , but the walls were timbered . the raiders piled straw against the wood and burnt them all alive . when the wendish folk opened their gates to flee the fire ,'\n",
      " b'and king \\' s landing , the iron throne and the seven kingdoms , all they have taken from us , we will have it back .\" viserys lived for that day . all that daenerys wanted back was the big house with the red door , the lemon tree outside'\n",
      " b'the past ,\" maester luwin said thoughtfully .\" he knew every stone in winterfell .\"\" gods ,\" robb swore , his young face dark with anger .\" if this is true , he will pay for it .\" he drew his sword and waved it in the air .\" i \\''\n",
      " b'\\' s brothers are also in the party ,\" she told him . ned grimaced at that . there was small love between him and the queen \\' s family , catelyn knew . the lannisters of casterly rock had come late to robert \\' s cause , when victory was'\n",
      " b', the lannisters on foot save for jaime himself . a charge might win them free , but it seemed to eddard stark that they had a surer , safer tactic .\" kill me ,\" he warned the kingslayer ,\" and catelyn will most certainly slay tyrion .\" jaime lannister poked'\n",
      " b'smoking earth .\" blood of my blood ,\" she heard aggo echo .\" blood of my blood ,\" rakharo shouted . and after them came her handmaids , and then the others , all the dothraki , men and women and children , and dany had only to look at their'\n",
      " b'to close the door . the wars went on until the earth ran red with blood of men and children both , but more children than men , for men were bigger and stronger , and wood and stone and obsidian make a poor match for bronze . finally the wise'\n",
      " b'on the kingsroad . the rain obscured the fields beyond the crossroads , but catelyn saw the land clear enough in her memory . the marketplace was just across the way , and the village a mile farther on , half a hundred white cottages surrounding a small stone sept .'\n",
      " b'was sixteen , tall and muscular , and his blows were as hard as any jon had ever felt .\" this will be uglier than a whore \\' s ass ,\" pyp muttered , and it was . page 172 the fight lasted less than a minute before the fat boy'\n",
      " b'that . jhiqui had taught her that a bloodrider was more than a guard ; they were the khal \\' s brothers , his shadows , his fiercest friends .\" blood of my blood ,\" drogo called them , and so it was ; they shared a single life . the'\n",
      " b'the most savory in all the seven kingdoms . best of all , she had no interest in their names .\" i think it best if you stay away from the common room ,\" ser rodrik said , after they had settled in .\" even in a place like this ,'\n",
      " b'\\' s tower alone , with a curious sense of apprehension . the brothers on guard eyed him solemnly as he approached .\" the old bear \\' s in his solar ,\" one of them announced . page 375\" he was asking for you .\" jon nodded . he should have'\n",
      " b'would fill up one by one as it grew colder . when the snow fell and the ice winds howled down out of the north , old nan said , farmers left their frozen fields and distant holdfasts , loaded up their wagons , and then the winter town came alive'\n",
      " b'to the queen again ,\" sansa told them , as she told everyone she saw that day .\" she \\' ll want to talk to me , i know she will . tell her i want to see her , please . if not the queen , then prince joffrey ,'\n",
      " b'on your face .\" there is no privacy in the heart of the khalasar . dany felt the eyes on her as she undressed him , heard the soft voices as she did the things that doreah had told her to do . it was nothing to her . was she'\n",
      " b'blazoned with the same striding huntsman he wore on his surcoat . as none of it was black , however , ser alliser insisted that he reequip himself from the armory . that took half the morning . his girth required donal noye to take apart a mail hauberk and refit'\n",
      " b'boar of crakehall , the bantam rooster of swyft , and more . his lord father took his place on the hill where he had slept . around him , the reserve assembled ; a huge force , half mounted and half foot , five thousand strong . lord tywin almost'\n",
      " b'of house arryn .\" it \\' s from lysa .\" catelyn looked at her husband .\" it will not make us glad ,\" she told him .\" there is grief in this message , ned . i can feel it .\" ned frowned , his face darkening .\" open it .\"'\n",
      " b\"his father , descending into the crypts . only this time the dream had gone further than before . in the dark he ' d heard the scrape of stone on stone . when he turned he saw that the vaults were opening , one after the other . as the\"], shape=(32,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "for batch in raw_train_ds.take(1):\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5bd836c-9bae-4d0a-ba59-69db03cd7ae9",
   "metadata": {},
   "source": [
    "- Definir tamaño de vocabulario y __vectorize_layer__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2bf228aa-9e3c-4422-a738-78f1380cb7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_size = 11994\n",
    "\n",
    "vectorize_layer = layers.TextVectorization(\n",
    "    standardize=None,\n",
    "    max_tokens=voc_size - 1,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=seq_length + 1,\n",
    "    #split='character'\n",
    ")\n",
    "\n",
    "vectorize_layer.adapt(raw_train_ds)\n",
    "vocab = vectorize_layer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc1d5d3a-fd63-4d4a-99fd-9ddf4d740e2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11993"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2fec7f6e-d550-4b0d-8901-d3500fabf479",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 51), dtype=int64, numpy=\n",
       "array([[   8, 1115,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0],\n",
       "       [   9, 1734,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0]])>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorize_layer(['a game', 'of thrones'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca673882-4178-436b-bc48-50bb6d848163",
   "metadata": {},
   "source": [
    "- Tokenizar palabras y obtener el texto objetivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "31d53600-2263-44bd-9ed2-4d30427d42b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_target(text):\n",
    "    tokenized_text = vectorize_layer(text)\n",
    "    input_text = tokenized_text[:, :-1]\n",
    "    target_text = tokenized_text[:, 1:]\n",
    "    return input_text, target_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "67152440-a6cb-4f45-93e0-be22c666a479",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = raw_train_ds.map(get_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c59be795-a130-47a7-ae5d-effec8a0363b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 50) (32, 50)\n",
      "tf.Tensor(\n",
      "[  98   11  738 1459   17    2 7285 1234   39  497    3  439    6 2710\n",
      "  515 2710  515 2710  195   48  616  999  563   39    3  291   52 2159\n",
      "   19  115   23  332   10   21    8  542   64   11 4134    4  269   13\n",
      "  143 5514   30  520    4   77 6499   24], shape=(50,), dtype=int64) tf.Tensor(\n",
      "[  11  738 1459   17    2 7285 1234   39  497    3  439    6 2710  515\n",
      " 2710  515 2710  195   48  616  999  563   39    3  291   52 2159   19\n",
      "  115   23  332   10   21    8  542   64   11 4134    4  269   13  143\n",
      " 5514   30  520    4   77 6499   24   31], shape=(50,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for input_batch, target_batch in train_ds.take(1):\n",
    "    print(input_batch.shape, target_batch.shape)\n",
    "    print(input_batch[0], target_batch[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238c3e71-f3b9-4503-be11-4fc80f56a114",
   "metadata": {},
   "source": [
    "## Definir modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8e8de9d6-1294-494d-ba6e-c61d65e0cdc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim = 256\n",
    "model_dim = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8daa37ac-29c2-42ba-ac7d-2c51817ba827",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(tf.keras.Model):\n",
    "    def __init__(self, voc_size, emb_dim, model_dim):\n",
    "        super().__init__(self)\n",
    "        self.embedding = layers.Embedding(voc_size, emb_dim)\n",
    "        self.gru = layers.GRU(model_dim,\n",
    "                              return_sequences=True,\n",
    "                              return_state=True)\n",
    "        self.logits = layers.Dense(voc_size)\n",
    "\n",
    "    def call(self, inputs, states=None, return_state=False, training=False):\n",
    "        x = inputs\n",
    "        x = self.embedding(x, training=training)\n",
    "        if states is None:\n",
    "            states = self.gru.get_initial_state(x)\n",
    "        x, states = self.gru(x, initial_state=states, training=training)\n",
    "        x = self.logits(x, training=training)\n",
    "\n",
    "        if return_state:\n",
    "            return x, states\n",
    "        else:\n",
    "            return x \n",
    "\n",
    "model = RNN(voc_size=voc_size,\n",
    "            emb_dim=emb_dim,\n",
    "            model_dim=model_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b766f872-c87b-4daa-ada6-5c10aa9fb0ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 50, 11994) (32, 50)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not load symbol cublasGetSmCountTarget from libcublas.so.11. Error: /usr/local/cuda/targets/x86_64-linux/lib/libcublas.so.11: undefined symbol: cublasGetSmCountTarget\n"
     ]
    }
   ],
   "source": [
    "for input_batch, target_batch in train_ds.take(1):\n",
    "    predictions = model(target_batch)\n",
    "    print(predictions.shape, target_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c82bbc39-47eb-4cba-8e9a-068cac936364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"rnn\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       multiple                  3070464   \n",
      "                                                                 \n",
      " gru (GRU)                   multiple                  3938304   \n",
      "                                                                 \n",
      " dense (Dense)               multiple                  12293850  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 19,302,618\n",
      "Trainable params: 19,302,618\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8690a5f9-0afc-4480-b174-df4dbd7bdfd8",
   "metadata": {},
   "source": [
    "- Salida del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "486b3233-c1d5-48d2-af0c-810e1bb1e609",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([50, 11994])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b7ae5628-d550-4769-84a1-4042b5b62438",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(50,), dtype=int64, numpy=\n",
       "array([10502,  2289,  4879,  4069,  8901,  1482,  6723, 11752,  1429,\n",
       "        9543, 11878,  7375,   371, 11941,  5616,  3622,  1491,  8269,\n",
       "        8400,  4338, 10961, 11025, 11466, 10839,  3300,   828, 10872,\n",
       "        1489,  9947,  1747, 11319,   122, 11746,  4858,  1084,  6851,\n",
       "       10897,  4616,   723,  3186,  3374,  9833,  4313,  6468, 11533,\n",
       "        7786,  4361,  6236,  3023,   366])>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_indices = tf.random.categorical(predictions[0], num_samples=1)\n",
    "pred_indices[:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df6be19-38b3-4ad7-ba16-5852acf791ba",
   "metadata": {},
   "source": [
    "- Obtener palabras a partir de índices con __vocab__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "986d5838-fad7-4602-9a5b-cbfc0997ad9e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"fermented mare ' s milk and illyrio ' s fine wines , and spat jests at each other across the fires , their voices harsh and alien in dany ' s ears . viserys was seated just below her , splendid in a new black wool tunic with a scarlet\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join([vocab[_] for _ in input_batch[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e16e77f4-e834-492b-bf2b-6d1c3ee1d779",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'discarded scars whinny eased promontories streets leaks 291\" whisper lawless 172 bethany ground 116 dreaded farewells ringing spearhead skilled skills bump boundary 552\" chuckle blur pushed chasings scared grooming seed ajar yet 297 wove royal heaved casual freely standing idea timbered heaviness stepping public 492 unmade seagard smolder wrists thing'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join([vocab[_] for _ in pred_indices[:, 0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504690e6-e7ba-4db2-b328-2edba7362aee",
   "metadata": {},
   "source": [
    "## Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1e5b0e6a-e58d-4217-8712-8b8bce417c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "loss_metric = tf.keras.metrics.Mean(name='loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "84bec055-4ba0-4e4f-a81c-5d01f3eb9126",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(input_batch, target_batch):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(input_batch, training=True)\n",
    "        loss_value = loss(target_batch, logits)\n",
    "\n",
    "    gradients = tape.gradient(loss_value, model.trainable_weights)\n",
    "    opt.apply_gradients(zip(gradients, model.trainable_weights))\n",
    "    loss_metric(loss_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f98822e4-a8ed-42e8-b0cb-f8d38be5b351",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "38644692-bf3f-4f2a-9812-4c7b9b80b7cd",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Loss: 7.126213550567627\n",
      "Epoch: 1 Loss: 6.49312162399292\n",
      "Epoch: 2 Loss: 6.39521598815918\n",
      "Epoch: 3 Loss: 6.2333784103393555\n",
      "Epoch: 4 Loss: 6.122547149658203\n",
      "Epoch: 5 Loss: 6.042852401733398\n",
      "Epoch: 6 Loss: 5.949287414550781\n",
      "Epoch: 7 Loss: 5.8541998863220215\n",
      "Epoch: 8 Loss: 5.778655529022217\n",
      "Epoch: 9 Loss: 5.717395782470703\n",
      "Epoch: 10 Loss: 5.660513401031494\n",
      "Epoch: 11 Loss: 5.6043243408203125\n",
      "Epoch: 12 Loss: 5.547576904296875\n",
      "Epoch: 13 Loss: 5.496826648712158\n",
      "Epoch: 14 Loss: 5.448652744293213\n",
      "Epoch: 15 Loss: 5.405211448669434\n",
      "Epoch: 16 Loss: 5.363319396972656\n",
      "Epoch: 17 Loss: 5.323212623596191\n",
      "Epoch: 18 Loss: 5.282846450805664\n",
      "Epoch: 19 Loss: 5.245517253875732\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    for input_batch, target_batch in train_ds:\n",
    "        train_step(input_batch, target_batch)\n",
    "        \n",
    "    print(f'Epoch: {epoch} Loss: {loss_metric.result().numpy()}')\n",
    "    loss_metric.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76012316-ca31-4f70-bd29-ec7ae54176a8",
   "metadata": {},
   "source": [
    "## Generación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9dde552e-90ab-447b-b64d-71169d225935",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tyrion windpipe , ? some and there would home ,\" a left in the seven said . he stupid not call for me ,\" arya admitted .\" he was night , and fire from wine .\" i want you .\" i go to the journey . truly can see he watched'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states = None\n",
    "start = 'tyrion'\n",
    "context = tf.constant([start])\n",
    "output = [start]\n",
    "\n",
    "for i in range(50):\n",
    "    #print(vectorize_layer(context)[:, :1])\n",
    "    # Obtener solo el primer elemento que regresa vectorize_layer\n",
    "    pred_logits, states = model(vectorize_layer(context)[:, :1], \n",
    "                                states=states, return_state=True)\n",
    "    #print(pred_logits.shape)\n",
    "    pred_index = tf.random.categorical(pred_logits[:, -1, :], \n",
    "                                       num_samples=1)\n",
    "\n",
    "    #print(vocab[pred_index[0, 0]])\n",
    "    context = tf.constant([vocab[pred_index[0, 0]]])\n",
    "    output.append(vocab[pred_index[0, 0]])\n",
    "    \n",
    "' '.join(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c55c04-efe1-4648-9845-90ca41ce6582",
   "metadata": {},
   "source": [
    "- Crear un vocabulario con todas las palabras del conjunto de datos es costoso. Esto obliga a reducir el número de palabras para el entrenamiento, limitando la capacidad del modelo. Es por eso que en la práctica se utilzian métodos como BPE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb2fc2b-10b2-438c-b394-3261c1d5e2fa",
   "metadata": {},
   "source": [
    "## Ejercicio\n",
    "- Incrementar el tamaño del dataset utilizando todos los libros de _A song of ice and fire_\n",
    "- Remplazar GRU por LSTM.\n",
    "- Utilizar otro método de Tokenización."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
