{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cffd618d-5bc8-43ea-a18b-66a484cb87ad",
   "metadata": {},
   "source": [
    "# Seq2seq\n",
    "- En este notebook se define una arquitectura seq2seq para traducir oraciones del inglés al español.\n",
    "\n",
    "<img src=\"../img/seq-to-seq.png\" width=\"700\"/>\n",
    "\n",
    "__Imagen tomada de Sutskever, I., Vinyals, O., & Le, Q. V. (2014). Sequence to sequence learning with neural networks. Advances in neural information processing systems, 27.__\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "843670d1-04f9-4a46-9f1c-fa0e1b4f92d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' # Disable tensorflow debugging logs\n",
    "import pathlib\n",
    "import random\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257dac3e-4b4f-4eac-964d-4949e9c7c070",
   "metadata": {},
   "source": [
    "## 1.- Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16c0293e-4292-4aef-82f8-d81a0a8b35fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = keras.utils.get_file(\n",
    "    fname=\"spa-eng.zip\",\n",
    "    origin=\"http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\",\n",
    "    extract=True,\n",
    ")\n",
    "text_file = pathlib.Path(text_file).parent / \"spa-eng\" / \"spa.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b71cea7-5bc5-4b94-8bc3-7cee706c0fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(text_file) as f:\n",
    "    lines = f.read().split(\"\\n\")[:-1]\n",
    "    \n",
    "text_pairs = []\n",
    "for line in lines:\n",
    "    eng, spa = line.split(\"\\t\")\n",
    "    spa = \"[start] \" + spa + \" [end]\"\n",
    "    text_pairs.append((eng, spa))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8956c567-5805-4435-94a0-453b90b498f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('This page has been intentionally left blank.', '[start] Esta página fue dejada en blanco intencionalmente. [end]')\n",
      "('Would you eat the last cookie on the plate if other people were watching?', '[start] ¿Te comerías la última galletita del plato si otra persona estuviese mirando? [end]')\n",
      "('We are men.', '[start] Somos hombres. [end]')\n",
      "(\"I realized I wasn't ready.\", '[start] Me di cuenta que no estaba lista. [end]')\n",
      "('Tom injured himself when he jumped out of the window.', '[start] Tom se lastimó cuando saltó por la ventana. [end]')\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    print(random.choice(text_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3cd4ad2-f792-4728-a61f-f10407f27f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118964 total pairs\n",
      "83276 training pairs\n",
      "17844 validation pairs\n",
      "17844 test pairs\n"
     ]
    }
   ],
   "source": [
    "random.shuffle(text_pairs)\n",
    "num_val_samples = int(0.15 * len(text_pairs))\n",
    "num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
    "train_pairs = text_pairs[:num_train_samples]\n",
    "val_pairs = text_pairs[num_train_samples : num_train_samples + num_val_samples]\n",
    "test_pairs = text_pairs[num_train_samples + num_val_samples :]\n",
    "\n",
    "print(f\"{len(text_pairs)} total pairs\")\n",
    "print(f\"{len(train_pairs)} training pairs\")\n",
    "print(f\"{len(val_pairs)} validation pairs\")\n",
    "print(f\"{len(test_pairs)} test pairs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6544172c-60cc-4039-b5ef-968fd3776012",
   "metadata": {},
   "source": [
    "## 2.- Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb61fb7f-164f-44d8-b974-1118f55f3f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "strip_chars = string.punctuation + \"¿\"\n",
    "strip_chars = strip_chars.replace(\"[\", \"\")\n",
    "strip_chars = strip_chars.replace(\"]\", \"\")\n",
    "\n",
    "vocab_size = 15000\n",
    "maxlen = 10\n",
    "batch_size = 64\n",
    "\n",
    "def custom_standardization(input_string):\n",
    "    lowercase = tf.strings.lower(input_string)\n",
    "    return tf.strings.regex_replace(lowercase, \"[%s]\" % re.escape(strip_chars), \"\")\n",
    "\n",
    "eng_vectorization = TextVectorization(\n",
    "    max_tokens=vocab_size, output_mode=\"int\", \n",
    "    output_sequence_length=maxlen,\n",
    ")\n",
    "spa_vectorization = TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=maxlen,\n",
    "    standardize=custom_standardization,\n",
    ")\n",
    "train_eng_texts = [pair[0] for pair in train_pairs]\n",
    "train_spa_texts = [pair[1] for pair in train_pairs]\n",
    "eng_vectorization.adapt(train_eng_texts)\n",
    "spa_vectorization.adapt(train_spa_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97d84341-8e27-46be-9fb3-a885633ba23e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 10), dtype=int64, numpy=\n",
       "array([[ 19, 233,   8,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [ 19, 173,   8,   0,   0,   0,   0,   0,   0,   0]])>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_vectorization([['my name is'], ['my dog is']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9089253b-7dcd-4371-8b55-91f3c9bdb707",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab556a43-d722-45a7-8a44-9f0df5c1d7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(eng, spa):\n",
    "    eng = eng_vectorization(eng)\n",
    "    spa = spa_vectorization(spa)\n",
    "    return tf.reverse(eng, [1]), spa[:, :-1], spa[:, 1:]\n",
    "\n",
    "\n",
    "def make_dataset(pairs):\n",
    "    eng_texts, spa_texts = zip(*pairs)\n",
    "    eng_texts = list(eng_texts)\n",
    "    spa_texts = list(spa_texts)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, spa_texts))\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.map(preprocess)\n",
    "    return dataset.shuffle(2048).prefetch(AUTOTUNE).cache()\n",
    "\n",
    "\n",
    "train_ds = make_dataset(train_pairs)\n",
    "val_ds = make_dataset(val_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e60e15f-35a7-496e-93e0-54c81128351d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([   0  487   12  495 1720  100  254    4  217    3], shape=(10,), dtype=int64) tf.Tensor([   2  399 1244   13 1120    6   17  574    4], shape=(9,), dtype=int64) tf.Tensor([ 399 1244   13 1120    6   17  574    4  101], shape=(9,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for inp_enc, inp_dec, tar_dec in train_ds.take(1):\n",
    "    print(inp_enc[0], inp_dec[0], tar_dec[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1220a01b-db62-412b-b160-d1e12547c6f3",
   "metadata": {},
   "source": [
    "## 3.- Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36cac235-72b2-4550-9b86-dc55ef68e21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim = 256\n",
    "model_dim = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f86988d-5d37-4f81-92f4-9dd5f2b8b661",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36ba4172-b3ac-429e-8904-defdccac7394",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(64, 512), dtype=float32, numpy=\n",
       "array([[ 0.01345986, -0.01282817,  0.00078651, ..., -0.00138945,\n",
       "        -0.00193483,  0.00081795],\n",
       "       [-0.00586681, -0.00865781,  0.00233197, ...,  0.00886651,\n",
       "         0.01459937,  0.01042136],\n",
       "       [-0.0097565 ,  0.01415568,  0.00180495, ..., -0.00564828,\n",
       "        -0.00428695, -0.00428702],\n",
       "       ...,\n",
       "       [-0.01033887, -0.00555763,  0.00349164, ..., -0.00078032,\n",
       "         0.00973689,  0.00733916],\n",
       "       [-0.00175286, -0.0080623 ,  0.00452699, ...,  0.00399484,\n",
       "         0.0069435 ,  0.01251333],\n",
       "       [ 0.00117152,  0.00446815,  0.00635694, ...,  0.00548611,\n",
       "         0.00730795,  0.01092208]], dtype=float32)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, voc_size, emb_dim, model_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = tf.keras.layers.Embedding(voc_size,\n",
    "                                                   emb_dim)\n",
    "        self.gru = tf.keras.layers.GRU(model_dim,\n",
    "                                       return_sequences=False,\n",
    "                                       return_state=True)\n",
    "        \n",
    "    def call(self, x, state=None):\n",
    "        x = self.embedding(x)\n",
    "        x, state = self.gru(x, initial_state=state)\n",
    "        return x, state\n",
    "    \n",
    "    \n",
    "encoder = Encoder(eng_vectorization.vocabulary_size(),\n",
    "                  emb_dim, model_dim)\n",
    "output, enc_state = encoder(inp_enc)\n",
    "enc_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2fd490ee-0b1b-4cdc-9893-692aaacda333",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 512])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "916c397c-61aa-4df8-a426-c21fc10d20ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       multiple                  3073024   \n",
      "                                                                 \n",
      " gru (GRU)                   multiple                  1182720   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,255,744\n",
      "Trainable params: 4,255,744\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478b656b-d6c8-4bd5-9139-8aba17babe4e",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a606a2c6-8069-4ac0-984d-f1177837fdd4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(64, 1, 15000), dtype=float32, numpy=\n",
       "array([[[-4.2371978e-03,  2.5792955e-03,  1.1927417e-03, ...,\n",
       "          1.4199783e-03, -1.8674907e-03,  4.3898684e-04]],\n",
       "\n",
       "       [[-3.1932073e-03, -3.2564742e-04, -2.0393071e-04, ...,\n",
       "          1.0287821e-03,  2.3111093e-03,  1.7360961e-03]],\n",
       "\n",
       "       [[-1.0320020e-03, -2.5059234e-03,  1.3493659e-03, ...,\n",
       "          1.5876164e-03,  1.4532279e-03,  1.5037364e-03]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-3.3585376e-03, -1.5357733e-03,  1.7665242e-04, ...,\n",
       "          8.2846957e-05, -2.3197941e-03,  9.5731323e-04]],\n",
       "\n",
       "       [[-3.1409599e-03, -9.0502808e-04,  1.8727144e-03, ...,\n",
       "          8.9994294e-04, -2.2213692e-03, -4.5712304e-04]],\n",
       "\n",
       "       [[-3.5127893e-03, -6.7628070e-04,  1.7415372e-03, ...,\n",
       "         -9.8129280e-04, -3.4677403e-03,  4.5821469e-04]]], dtype=float32)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, voc_size, emb_dim, model_dim):\n",
    "        super().__init__(self)\n",
    "        self.embedding = layers.Embedding(voc_size, emb_dim)\n",
    "        self.gru = layers.GRU(model_dim,\n",
    "                              return_sequences=True,\n",
    "                              return_state=True)\n",
    "        self.logits = layers.Dense(voc_size)\n",
    "\n",
    "    def call(self, x, states, return_state=False, training=False):\n",
    "        x = self.embedding(x, training=training)\n",
    "        x, states = self.gru(x, initial_state=states, training=training)\n",
    "        x = self.logits(x, training=training)\n",
    "\n",
    "        if return_state:\n",
    "            return x, states\n",
    "        else:\n",
    "            return x \n",
    "\n",
    "\n",
    "decoder = Decoder(voc_size=spa_vectorization.vocabulary_size(),\n",
    "                  emb_dim=emb_dim,\n",
    "                  model_dim=model_dim)\n",
    "\n",
    "decoder(inp_dec[:, :1], enc_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d6fd6976-05a1-446b-be33-db7b8747035e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     multiple                  3840000   \n",
      "                                                                 \n",
      " gru_1 (GRU)                 multiple                  1182720   \n",
      "                                                                 \n",
      " dense (Dense)               multiple                  7695000   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 12,717,720\n",
      "Trainable params: 12,717,720\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d773b6cc-1f5c-4251-bf66-d810aba70357",
   "metadata": {},
   "source": [
    "## 4.- Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "87162be7-ffe4-41d8-9385-db9ae41f0715",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(0.001)\n",
    "\n",
    "def loss_function(label, pred):\n",
    "    mask = label != 0\n",
    "    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "    loss = loss_object(label, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss.dtype)\n",
    "    loss *= mask\n",
    "\n",
    "    loss = tf.reduce_sum(loss)/tf.reduce_sum(mask)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bad6a29b-b44b-4b83-8f28-50ad712cba3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5ad8ca11-06d8-4cd1-bfc2-bb5e3c56236f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[   0    0 1743    4 6271  492   43  743  729    2]\n",
      " [   0    0   90  789    2   10  349  925    8  152]\n",
      " [   0    0    0    0    0 3384    7  913    8    6]], shape=(3, 10), dtype=int64) tf.Tensor(\n",
      "[[   2   19  417  424  264 2664    1   22 2140]\n",
      " [   2  809   39  238    9  582   11    9  565]\n",
      " [   2   94  296   12 5434    3    0    0    0]], shape=(3, 9), dtype=int64) tf.Tensor(\n",
      "[[  19  417  424  264 2664    1   22 2140    3]\n",
      " [ 809   39  238    9  582   11    9  565   11]\n",
      " [  94  296   12 5434    3    0    0    0    0]], shape=(3, 9), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for inp_enc, inp_dec, tar_dec in train_ds.take(1):\n",
    "    print(inp_enc[:3], inp_dec[:3], tar_dec[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8c95d3d9-c4e9-4400-bb4b-57d54f313f30",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([64, 512]), TensorShape([64, 9]), TensorShape([64, 9]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, state = encoder(inp_enc, training=True)\n",
    "state.shape, inp_dec.shape, tar_dec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ea2bd42b-e98f-4abb-9d04-324d55b6ef44",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp_enc, inp_dec, tar_dec):\n",
    "    with tf.GradientTape() as tape:\n",
    "        _, state = encoder(inp_enc, training=True)\n",
    "        pred = decoder(inp_dec, state, training=True)\n",
    "        loss_value = loss_function(tar_dec, pred)\n",
    "        \n",
    "    weights = encoder.trainable_weights + decoder.trainable_weights\n",
    "    gradients = tape.gradient(loss_value, weights)\n",
    "    optimizer.apply_gradients(zip(gradients, weights))\n",
    "    train_loss(loss_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3a5b1064-ba11-489e-8767-f82cfbc6f34b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ids_to_text = tf.keras.layers.StringLookup(\n",
    "                vocabulary=spa_vectorization.get_vocabulary(),\n",
    "                mask_token='',\n",
    "                invert=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "983248bf-3db4-4263-bc4e-e29bce73df9c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "sentences = ['i love my dog',\n",
    "             'i love to sleep',\n",
    "             'the cat wants to eat']\n",
    "\n",
    "def print_translation(sentence):\n",
    "    inp = eng_vectorization([sentence])\n",
    "    inp = tf.reverse(inp, [1])\n",
    "    _, state = encoder(inp, training=False)\n",
    "    dec_inp = spa_vectorization(['[start]'])[:, :1]\n",
    "    output = []\n",
    "    pred_index = ''\n",
    "\n",
    "    while pred_index != '[end]':\n",
    "        logits, state = decoder(dec_inp, state, return_state=True, training=False)\n",
    "        dec_inp = tf.argmax(logits, axis=-1)\n",
    "        pred_index = ids_to_text(dec_inp)\n",
    "        output.append(pred_index[0][0].numpy().decode('utf-8'))\n",
    "\n",
    "    text = ' '.join(output[:-1])\n",
    "    print(f'Input: {sentence}')\n",
    "    print(f'Prediction: {text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9cd8c900-a636-40c5-bf51-0bae31e81894",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time taken for epoch 1 is: 19.94 secs Loss: 5.0653\n",
      "Input: i love my dog\n",
      "Prediction: no puedo creer que tom no es tan bueno\n",
      "Input: i love to sleep\n",
      "Prediction: no puedo creer que tom no es tan bueno\n",
      "Input: the cat wants to eat\n",
      "Prediction: el tren se detuvo\n",
      "\n",
      "Time taken for epoch 2 is: 7.22 secs Loss: 3.7809\n",
      "Input: i love my dog\n",
      "Prediction: me gusta el helado de tom\n",
      "Input: i love to sleep\n",
      "Prediction: yo estaba [UNK]\n",
      "Input: the cat wants to eat\n",
      "Prediction: el hombre se detuvo a su madre\n",
      "\n",
      "Time taken for epoch 3 is: 7.29 secs Loss: 2.7501\n",
      "Input: i love my dog\n",
      "Prediction: me encanta el helado\n",
      "Input: i love to sleep\n",
      "Prediction: me encanta la sandía\n",
      "Input: the cat wants to eat\n",
      "Prediction: el perro quiere ir\n",
      "\n",
      "Time taken for epoch 4 is: 7.62 secs Loss: 1.9537\n",
      "Input: i love my dog\n",
      "Prediction: adoro mi perro\n",
      "Input: i love to sleep\n",
      "Prediction: me encanta dormir\n",
      "Input: the cat wants to eat\n",
      "Prediction: el perro quiere ir\n",
      "\n",
      "Time taken for epoch 5 is: 6.62 secs Loss: 1.4104\n",
      "Input: i love my dog\n",
      "Prediction: me gusta el perro\n",
      "Input: i love to sleep\n",
      "Prediction: me gusta dormir\n",
      "Input: the cat wants to eat\n",
      "Prediction: el café quiere ser salvada\n",
      "\n",
      "Time taken for epoch 6 is: 7.34 secs Loss: 1.0458\n",
      "Input: i love my dog\n",
      "Prediction: adoro mi perro\n",
      "Input: i love to sleep\n",
      "Prediction: me gusta dormir\n",
      "Input: the cat wants to eat\n",
      "Prediction: el gato quiere comer\n"
     ]
    }
   ],
   "source": [
    "epochs = 7\n",
    "\n",
    "for epoch in range(1, epochs):\n",
    "    start = time.time()\n",
    "    for inp_enc, inp_dec, tar_dec in train_ds:\n",
    "        train_step(inp_enc, inp_dec, tar_dec)\n",
    "        \n",
    "    print(f'\\nTime taken for epoch {epoch} is: {time.time() - start:.2f} secs', end=' ')\n",
    "    print(f'Loss: {train_loss.result():.4f}')\n",
    "    train_loss.reset_states()\n",
    "    \n",
    "    for s in sentences:\n",
    "        print_translation(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfa7858-1f2a-492f-8a55-a454c2f3ae2a",
   "metadata": {},
   "source": [
    "## Ejercicio\n",
    "- Agregar loop de evaluación.\n",
    "- Mejorar el modelo con las técnicas propuestas en _Sutskever, I., Vinyals, O., & Le, Q. V. (2014). Sequence to sequence learning with neural networks. Advances in neural information processing systems, 27._\n",
    "- Agreagar mecanismo de atención de _Bahdanau_."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
