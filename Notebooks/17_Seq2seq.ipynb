{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cffd618d-5bc8-43ea-a18b-66a484cb87ad",
   "metadata": {},
   "source": [
    "# Seq2seq\n",
    "- En este notebook se define una arquitectura seq2seq para traducir oraciones del inglés al español.\n",
    "\n",
    "<img src=\"../img/seq-to-seq.png\" width=\"700\"/>\n",
    "\n",
    "__Imagen tomada de Sutskever, I., Vinyals, O., & Le, Q. V. (2014). Sequence to sequence learning with neural networks. Advances in neural information processing systems, 27.__\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "843670d1-04f9-4a46-9f1c-fa0e1b4f92d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' # Disable tensorflow debugging logs\n",
    "import pathlib\n",
    "import random\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257dac3e-4b4f-4eac-964d-4949e9c7c070",
   "metadata": {},
   "source": [
    "## 1.- Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16c0293e-4292-4aef-82f8-d81a0a8b35fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = keras.utils.get_file(\n",
    "    fname=\"spa-eng.zip\",\n",
    "    origin=\"http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\",\n",
    "    extract=True,\n",
    ")\n",
    "text_file = pathlib.Path(text_file).parent / \"spa-eng\" / \"spa.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b71cea7-5bc5-4b94-8bc3-7cee706c0fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(text_file) as f:\n",
    "    lines = f.read().split(\"\\n\")[:-1]\n",
    "    \n",
    "text_pairs = []\n",
    "for line in lines:\n",
    "    eng, spa = line.split(\"\\t\")\n",
    "    spa = \"[start] \" + spa + \" [end]\"\n",
    "    text_pairs.append((eng, spa))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8956c567-5805-4435-94a0-453b90b498f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Tom collected stamps.', '[start] Tom coleccionaba sellos. [end]')\n",
      "('He repeated his question.', '[start] Él repitió su pregunta. [end]')\n",
      "(\"I understand the sentence, but I'm not able to translate it.\", '[start] Yo entiendo la frase, pero no logro traducirla. [end]')\n",
      "('Tom persuaded Mary to help John.', '[start] Tom persuadió a Mary para ayudar a John. [end]')\n",
      "('His jokes are not funny at all.', '[start] Sus chistes no son para nada graciosos. [end]')\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    print(random.choice(text_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3cd4ad2-f792-4728-a61f-f10407f27f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118964 total pairs\n",
      "83276 training pairs\n",
      "17844 validation pairs\n",
      "17844 test pairs\n"
     ]
    }
   ],
   "source": [
    "random.shuffle(text_pairs)\n",
    "num_val_samples = int(0.15 * len(text_pairs))\n",
    "num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
    "train_pairs = text_pairs[:num_train_samples]\n",
    "val_pairs = text_pairs[num_train_samples : num_train_samples + num_val_samples]\n",
    "test_pairs = text_pairs[num_train_samples + num_val_samples :]\n",
    "\n",
    "print(f\"{len(text_pairs)} total pairs\")\n",
    "print(f\"{len(train_pairs)} training pairs\")\n",
    "print(f\"{len(val_pairs)} validation pairs\")\n",
    "print(f\"{len(test_pairs)} test pairs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6544172c-60cc-4039-b5ef-968fd3776012",
   "metadata": {},
   "source": [
    "## 2.- Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb61fb7f-164f-44d8-b974-1118f55f3f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "strip_chars = string.punctuation + \"¿\"\n",
    "strip_chars = strip_chars.replace(\"[\", \"\")\n",
    "strip_chars = strip_chars.replace(\"]\", \"\")\n",
    "\n",
    "vocab_size = 15000\n",
    "maxlen = 10\n",
    "batch_size = 64\n",
    "\n",
    "def custom_standardization(input_string):\n",
    "    lowercase = tf.strings.lower(input_string)\n",
    "    return tf.strings.regex_replace(lowercase, \"[%s]\" % re.escape(strip_chars), \"\")\n",
    "\n",
    "eng_vectorization = TextVectorization(\n",
    "    max_tokens=vocab_size, output_mode=\"int\", \n",
    "    output_sequence_length=maxlen,\n",
    ")\n",
    "spa_vectorization = TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=maxlen,\n",
    "    standardize=custom_standardization,\n",
    ")\n",
    "train_eng_texts = [pair[0] for pair in train_pairs]\n",
    "train_spa_texts = [pair[1] for pair in train_pairs]\n",
    "eng_vectorization.adapt(train_eng_texts)\n",
    "spa_vectorization.adapt(train_spa_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97d84341-8e27-46be-9fb3-a885633ba23e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 10), dtype=int64, numpy=\n",
       "array([[ 19, 235,   8,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [ 19, 178,   8,   0,   0,   0,   0,   0,   0,   0]])>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_vectorization([['my name is'], ['my dog is']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9089253b-7dcd-4371-8b55-91f3c9bdb707",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab556a43-d722-45a7-8a44-9f0df5c1d7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(eng, spa):\n",
    "    eng = eng_vectorization(eng)\n",
    "    spa = spa_vectorization(spa)\n",
    "    return tf.reverse(eng, [1]), spa[:, :-1], spa[:, 1:]\n",
    "\n",
    "\n",
    "def make_dataset(pairs):\n",
    "    eng_texts, spa_texts = zip(*pairs)\n",
    "    eng_texts = list(eng_texts)\n",
    "    spa_texts = list(spa_texts)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, spa_texts))\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.map(preprocess)\n",
    "    return dataset.shuffle(2048).prefetch(AUTOTUNE).cache()\n",
    "\n",
    "\n",
    "train_ds = make_dataset(train_pairs)\n",
    "val_ds = make_dataset(val_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e60e15f-35a7-496e-93e0-54c81128351d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([   0    0    0    0    0    0 1023    2    5   23], shape=(10,), dtype=int64) tf.Tensor([  2 132  10 987   3   0   0   0   0], shape=(9,), dtype=int64) tf.Tensor([132  10 987   3   0   0   0   0   0], shape=(9,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for inp_enc, inp_dec, tar_dec in train_ds.take(1):\n",
    "    print(inp_enc[0], inp_dec[0], tar_dec[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1220a01b-db62-412b-b160-d1e12547c6f3",
   "metadata": {},
   "source": [
    "## 3.- Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36cac235-72b2-4550-9b86-dc55ef68e21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim = 256\n",
    "model_dim = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f86988d-5d37-4f81-92f4-9dd5f2b8b661",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36ba4172-b3ac-429e-8904-defdccac7394",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(64, 1024), dtype=float32, numpy=\n",
       "array([[ 0.00452388,  0.0023647 , -0.0015519 , ...,  0.00635016,\n",
       "         0.00875575,  0.01872421],\n",
       "       [ 0.00123157, -0.00115991, -0.00740096, ..., -0.00825757,\n",
       "        -0.00251782, -0.00243315],\n",
       "       [-0.00630107, -0.00409868, -0.00735846, ...,  0.00684062,\n",
       "         0.00467536, -0.00659731],\n",
       "       ...,\n",
       "       [-0.00764129, -0.01092764,  0.00017454, ...,  0.01025986,\n",
       "        -0.00862891, -0.01394198],\n",
       "       [-0.00279094, -0.00529953, -0.00042023, ..., -0.00011713,\n",
       "         0.00366927, -0.00757017],\n",
       "       [ 0.00152143,  0.00721026, -0.00376425, ..., -0.00796608,\n",
       "         0.00185755,  0.00385784]], dtype=float32)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, voc_size, emb_dim, model_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = tf.keras.layers.Embedding(voc_size,\n",
    "                                                   emb_dim)\n",
    "        self.gru = tf.keras.layers.GRU(model_dim,\n",
    "                                       return_sequences=False,\n",
    "                                       return_state=True)\n",
    "        \n",
    "    def call(self, x, state=None):\n",
    "        x = self.embedding(x)\n",
    "        x, state = self.gru(x, initial_state=state)\n",
    "        return x, state\n",
    "    \n",
    "    \n",
    "encoder = Encoder(eng_vectorization.vocabulary_size(),\n",
    "                  emb_dim, model_dim)\n",
    "output, enc_state = encoder(inp_enc)\n",
    "enc_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2fd490ee-0b1b-4cdc-9893-692aaacda333",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 1024])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "916c397c-61aa-4df8-a426-c21fc10d20ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       multiple                  3092992   \n",
      "                                                                 \n",
      " gru (GRU)                   multiple                  3938304   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7,031,296\n",
      "Trainable params: 7,031,296\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478b656b-d6c8-4bd5-9139-8aba17babe4e",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a606a2c6-8069-4ac0-984d-f1177837fdd4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(64, 1, 15000), dtype=float32, numpy=\n",
       "array([[[-8.50444310e-04, -7.19906110e-03,  2.85117159e-04, ...,\n",
       "         -1.43465970e-03,  8.48916388e-05,  2.58170650e-03]],\n",
       "\n",
       "       [[-7.43264391e-04, -5.14982687e-03, -1.91092945e-03, ...,\n",
       "         -1.62706245e-03, -9.96784511e-05,  2.18255562e-03]],\n",
       "\n",
       "       [[-2.51733552e-04, -4.67995415e-03, -5.42414316e-04, ...,\n",
       "         -2.83832848e-03,  2.62653339e-03,  2.64436705e-03]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-1.07496955e-04, -5.61239943e-03, -1.89118064e-03, ...,\n",
       "         -3.06227896e-03,  1.09169065e-04,  3.16094514e-03]],\n",
       "\n",
       "       [[-1.70495559e-03, -5.15167974e-03, -2.47775181e-03, ...,\n",
       "         -2.03523342e-03, -1.08800930e-04,  3.08055524e-03]],\n",
       "\n",
       "       [[-1.09492219e-03, -4.69784671e-03, -1.91600493e-03, ...,\n",
       "         -1.51437439e-03, -4.36020200e-04,  3.40514031e-04]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, voc_size, emb_dim, model_dim):\n",
    "        super().__init__(self)\n",
    "        self.embedding = layers.Embedding(voc_size, emb_dim)\n",
    "        self.gru = layers.GRU(model_dim,\n",
    "                              return_sequences=True,\n",
    "                              return_state=True)\n",
    "        self.logits = layers.Dense(voc_size)\n",
    "\n",
    "    def call(self, x, states, return_state=False, training=False):\n",
    "        x = self.embedding(x, training=training)\n",
    "        x, states = self.gru(x, initial_state=states, training=training)\n",
    "        x = self.logits(x, training=training)\n",
    "\n",
    "        if return_state:\n",
    "            return x, states\n",
    "        else:\n",
    "            return x \n",
    "\n",
    "\n",
    "decoder = Decoder(voc_size=spa_vectorization.vocabulary_size(),\n",
    "                  emb_dim=emb_dim,\n",
    "                  model_dim=model_dim)\n",
    "\n",
    "decoder(inp_dec[:, :1], enc_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d6fd6976-05a1-446b-be33-db7b8747035e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     multiple                  3840000   \n",
      "                                                                 \n",
      " gru_1 (GRU)                 multiple                  3938304   \n",
      "                                                                 \n",
      " dense (Dense)               multiple                  15375000  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 23,153,304\n",
      "Trainable params: 23,153,304\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d773b6cc-1f5c-4251-bf66-d810aba70357",
   "metadata": {},
   "source": [
    "## 4.- Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "87162be7-ffe4-41d8-9385-db9ae41f0715",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(0.001)\n",
    "\n",
    "def loss_function(label, pred):\n",
    "    mask = label != 0\n",
    "    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "    loss = loss_object(label, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss.dtype)\n",
    "    loss *= mask\n",
    "\n",
    "    loss = tf.reduce_sum(loss)/tf.reduce_sum(mask)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bad6a29b-b44b-4b83-8f28-50ad712cba3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5ad8ca11-06d8-4cd1-bfc2-bb5e3c56236f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[   0    0  841 1071   20  404   30    4   82    5]\n",
      " [   0    0    0    0    0    0  180   55 5107    3]\n",
      " [   0    0    0    0   45   41    4 3092    8    9]], shape=(3, 10), dtype=int64) tf.Tensor(\n",
      "[[   2   92    5  126  511   16  387  303    5]\n",
      " [   2   14  810   55    3    0    0    0    0]\n",
      " [   2   22 3701   16   63    3    0    0    0]], shape=(3, 9), dtype=int64) tf.Tensor(\n",
      "[[  92    5  126  511   16  387  303    5 3470]\n",
      " [  14  810   55    3    0    0    0    0    0]\n",
      " [  22 3701   16   63    3    0    0    0    0]], shape=(3, 9), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for inp_enc, inp_dec, tar_dec in train_ds.take(1):\n",
    "    print(inp_enc[:3], inp_dec[:3], tar_dec[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8c95d3d9-c4e9-4400-bb4b-57d54f313f30",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([64, 1024]), TensorShape([64, 9]), TensorShape([64, 9]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, state = encoder(inp_enc, training=True)\n",
    "state.shape, inp_dec.shape, tar_dec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ea2bd42b-e98f-4abb-9d04-324d55b6ef44",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp_enc, inp_dec, tar_dec):\n",
    "    with tf.GradientTape() as tape:\n",
    "        _, state = encoder(inp_enc, training=True)\n",
    "        pred = decoder(inp_dec, state, training=True)\n",
    "        loss_value = loss_function(tar_dec, pred)\n",
    "        \n",
    "    weights = encoder.trainable_weights + decoder.trainable_weights\n",
    "    gradients = tape.gradient(loss_value, weights)\n",
    "    optimizer.apply_gradients(zip(gradients, weights))\n",
    "    train_loss(loss_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3a5b1064-ba11-489e-8767-f82cfbc6f34b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ids_to_text = tf.keras.layers.StringLookup(\n",
    "                vocabulary=spa_vectorization.get_vocabulary(),\n",
    "                mask_token='',\n",
    "                invert=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "983248bf-3db4-4263-bc4e-e29bce73df9c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "sentences = ['i love my dog',\n",
    "             'i love to sleep',\n",
    "             'the cat wants to eat']\n",
    "\n",
    "def generate_translation(sentence):\n",
    "    inp = eng_vectorization([sentence])\n",
    "    inp = tf.reverse(inp, [1])\n",
    "    _, state = encoder(inp, training=False)\n",
    "    dec_inp = spa_vectorization(['[start]'])[:, :1]\n",
    "    output = []\n",
    "    pred_index = ''\n",
    "\n",
    "    while pred_index != '[end]':\n",
    "        logits, state = decoder(dec_inp, state, return_state=True, training=False)\n",
    "        dec_inp = tf.argmax(logits, axis=-1)\n",
    "        pred_index = ids_to_text(dec_inp)\n",
    "        output.append(pred_index[0][0].numpy().decode('utf-8'))\n",
    "\n",
    "    text = ' '.join(output[:-1])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9cd8c900-a636-40c5-bf51-0bae31e81894",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for epoch 1 is: 23.94 secs Loss: 4.3734\n",
      "Output: \n",
      "me gusta mi perro\n",
      "me encantan las galletas\n",
      "la policía quiere que [UNK]\n",
      "Time taken for epoch 2 is: 10.79 secs Loss: 2.5515\n",
      "Output: \n",
      "me encanta mi perro\n",
      "me encantan las noches\n",
      "el policía quiere comer\n",
      "Time taken for epoch 3 is: 10.75 secs Loss: 1.5755\n",
      "Output: \n",
      "amo a mi perro\n",
      "me gusta dormir\n",
      "el gato quiere comer\n",
      "Time taken for epoch 4 is: 10.83 secs Loss: 0.9843\n",
      "Output: \n",
      "amo a mi perro\n",
      "me gusta dormir\n",
      "el gato quiere comer\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "\n",
    "for epoch in range(1, epochs):\n",
    "    start = time.time()\n",
    "    for inp_enc, inp_dec, tar_dec in train_ds:\n",
    "        train_step(inp_enc, inp_dec, tar_dec)\n",
    "        \n",
    "    print(f'Time taken for epoch {epoch} is: {time.time() - start:.2f} secs', end=' ')\n",
    "    print(f'Loss: {train_loss.result():.4f}')\n",
    "    train_loss.reset_states()\n",
    "    \n",
    "    print('Output: ')\n",
    "    for s in sentences:\n",
    "        trans = generate_translation(s)\n",
    "        print(f\"{trans}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfa7858-1f2a-492f-8a55-a454c2f3ae2a",
   "metadata": {},
   "source": [
    "## Ejercicio\n",
    "- Agregar loop de evaluación.\n",
    "- Mejorar el modelo con las técnicas propuestas en _Sutskever, I., Vinyals, O., & Le, Q. V. (2014). Sequence to sequence learning with neural networks. Advances in neural information processing systems, 27._\n",
    "- Agreagar mecanismo de atención de _Bahdanau_."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
